\documentclass[a4paper,12pt]{article}
\usepackage{amsfonts}
\usepackage{tcolorbox}
\begin{document}

\title{Awkward State Machines}
\author{Will Dengler}
\maketitle

\section{Introduction}
Imagine you had a ball of yarn of infinite length and a pair of scizzors. Now choose some arbitrary length of yarn and cut it - we'll refer to this length as 1. Next, cut a second strand of yarn that has length 2 relative to your original piece of yarn and set it aside. Now, extend the yarn to length 3 and repeat the following instructions:
\begin{enumerate}
\item For every strand of yarn you have cut so far (other than the strand of length 1), check if the current extension of yarn can be split into even segments of the same length as the current strand by 'walking' the shorter strand up the longer one until you reach the end of or pass the end of the extended piece of yarn; if you reach the end of the extended piece exactly, then the extension can be divided into even segments. If none of the previous strands can be used to divide the current extension evenly, then double the length of the extended peice of yarn, then cut it in half, finally, set the cut strand aside with the others. Do not cut the extension if one of the previous strands does divide the extension evenly.
\item Using your strand of length 1, increase the length of the extension by 1.
\item Repeat the above two steps.
\end{enumerate} 

If you follow the above instructions, then order the strands of yarn you cut by their length, and finally wrote out their lengths relative to the strand of length 1, then you will find yourself writing down the prime numbers in consequetive order. The fact that this simple experiment derives the prime numbers using relative distance has always fascinated me. I've always had an itching notion that an algorithm based solely on relative distance, rather than integer arithmetic, would outperform the standard methods for discovering the primes in consequetive order. However, the problem of how to encode distance and divisibility without actually using numbers seemed to be impossible; after all, how do you tell a computer to use a ball of yarn and scizzors? And even if we could, the act of walking the strands up the extension is going to be pretty slow. 

Let's modify our experiment slightly to include tacks and a corkboard. Start again by cutting some length of yarn for defining length 1. Now, using your piece of yarn, seperate two tacks on your board 1 unit away from one another. You can now determine the strand of length two by wrapping your ball of yarn around the tacks such that you start at one tack, wrap around the second, and then return to the first tack, then cut the yarn to produce the strand of length two. Now use the strand of length two to place two more tacks in your board at 2 units apart.Now using the tacks for length 1, create a strand of length 3 and repeat the following instructions:   
\begin{enumerate}
\item For every pair of tacks on the board (other than the unit tacks), wrap the current extension of yarn around the tacks to check for divisibility. The extension is divisible if the yarn perfectly touches one of the tacks when it runs out of length. If none of the current pairs of tacks divide the extension evenly, then double the extension, cut it in half, and use the new strand to set apart a new pair of tacks on your board.
\item Use the unit pair of tacks to increase the length of the extension by 1.
\item Repeat the above two steps.
\end{enumerate} 

The above experiment is very simliar to the first. The pairs of tacks you've placed will wind up enumerating the primes once again. We are also still stuck with the problem of how could we encode this algorithm without integers, and the algorithm still isn't performing very quickly. However, the algorithm does highlight an important concept, our tacks use circles (technically ovals) in order to check the length of the extended piece of yarn. This is amazing because it allows us to check for divisiblity without counting anything out, instead, we just keep wrapping the yarn around until we get to the end; thus we don't need to know \textit{how} many times we've wrapped the yarn around, we just have to look where it winds up at the end.   

Let's modify our experiment once again. This time, you need tacks, a corkboard, and little flags that you can stick into the board (a needle with red tape for example). Start by placing two tacks down on the leftmost side of the board in a vertical line, and place a flag next to the bottom tack. Next, place three tacks in a vertical line to the right of those (imagine you are creating a bar graph with the tacks) and place a flag next to the highest tack. This expirement will require you to keep the vertical lines you create with the tacks distinct from one another, we shall refer to each line as a bar. Each bar in your board will always have a flag next to one of it's tacks. Repeat the following to run the experiment:
\begin{enumerate}
\item For every bar except the rightmost, move the flag to the tack above the one it is currently at; unless the flag is currently at the topmost tack in the bar, then move the flag to the bottom tack in the bar.
\item If there are no flags on any of the bottom tacks after you've moved every bar (other than the rightmost), then move the flag of the rightmost bar to its bottom tack and then create a new bar to the right of it that has one more tack than it, and place the new bar's flag at it's top tack.  
\item If there was a flag on one of the bottom tacks, then add a new tack to the top of the rightmost bar, and place it's flag next to the new tack.
\item Repeat the above three steps. 
\end{enumerate}

If you run the above experiment and then count the number of tacks in each bar, you will find that the number of tacks in the bars enumerate the prime numbers in consequetive order. How did this happen? Well, this experiment is much more similar to the last two than it first appears. In this case, the way we move the flags on each bar is the same as wrapping the extended piece of yarn around the tacks in experiment two, or 'walking' the yarn up the extended piece of yarn from experiment one. However, this time our flags our able to preserve the state from the last iteration since you just have to move the flags one tack forward. The moving of each tack can be done in constant time for each bar, and is incredibly fast relative to the time it takes to re-wrap the extended strand of yarn around the tacks, or perform the 'walks'. Furthermore, this expirement doesn't need distance; rather, it only needs relative position (bottom tack, tack above that, ..., tack below top tack, top tack). By replacing distance with relative position, we can easily encode our experiment without the use of numbers using cycle graphs implemented via linked lists. 
  
\section{Cycle Graphs}
In order for us to study awkward state machines in any detail, we are going to need to understand their underlying components. As such, let us start by examining the simplest component in an awkward state machine: the cycle graph.
\\
\begin{tcolorbox}
\textbf{Definition}\\
A \textit{cycle graph} is a directed, connected graph whose points form a circle. More explicity, for any $n > 1$, the cycle graph $G_n$ has points:
\begin{center}
$P = \{$ $p_i$ $|$ $0 \leq i < n$ $\}$
\end{center}
such that for any $i < n - 1$, $p_i$ has only one edge which goes to $p_{i + 1}$, and $p_{n-1}$ has only one edge going to $p_0$.
\end{tcolorbox}
\noindent
\\
Our definition tells us that a cycle graph is nothing more than a circular, directed graph with an explicit and straightforward labeling of its points. Our study of cycle graphs will be predominantly focused on the outcomes of walks about cycle graphs. As such, for the sake of clarity in our writing, let us now take the time to define two functions - the \textit{walk function} $\omega$ and the \textit{index walk function} $\Omega$ - that we can use to talk about walks of arbitrary length around any cycle graph.\\
\begin{tcolorbox}
\textbf{Definition}\\
For any cycle graph $G_n$ with points $P$, we define the \textit{walk function} $\omega: (P, \mathbb{N}^{+}) \rightarrow P$ to be $\omega(p_i, k) = p_j$, where $p_j$ is the point arrived at after a walk of length $k$ when starting at point $p_i$ on cycle graph $G_n$.\\
\\
Furthermore, we define the \textit{walk index function}, $\Omega: (P, \mathbb{N}^{+}) \rightarrow [n]$, $[n] = \{$ $i \in \mathbb{N}$ $|$ $0 \leq i < n$ $\}$ to be $\Omega(p_i, k) = j$, where $j$ is the index of the point $\omega(p_i, k)$.
\end{tcolorbox}
\noindent
\\
We already know a little bit about the walk and walk index functions from our definition of a cycle graph. Our definition tells us point that $p_{n-1}$ only has one edge going to point $p_0$. Furthermore, for any other point $p_k$ there is only a single edge going to point $p_{k+1}$. Thus, we have:
\begin{center}
$\omega(p_{n-1}, 1) = p_0$ and $\Omega(p_{n-1}, 1) = 0$\\
$\omega(p_k, 1) = p_{k+1}$ and $\Omega(p_k, 1) = k + 1$ for any $k < n - 1$\\
\end{center}
\noindent
In order for our mappings $\omega$ and $\Omega$ to be well defined, they will have defined for all input; furthermore, each input can only map to a single output. Since our functions map to the outcomes of walks of varying length around cycle graphs, they will only be valid if for any walk length, starting at any point, on any cycle graph, that it must be the case that the walk can be completed; furthermore, there must only be a single walk that can be taken. As such, we shall begin our study by proving just this.\\
\\
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any point $p_i$ within that graph, there is only a single walk of length $k$ that begins at point $p_i$.\\
\\
In terms of our walk functions, this lemma states that $\omega$ and $\Omega$ are defined for any point $p_i$ and any walk length $k$.\\
\\
More simply stated, $\omega$ and $\Omega$ are valid mappings.
\end{tcolorbox}
\noindent
\textit{Proof}\\
The truth to this lemma is result of the fact that every point in a cycle graph only has a single edge. As such, we shall use this fact to complete a proof by induction on the length of our walk.\\
\\
\textit{Base Case}\\ 
Our base case demands us to show that there is only a single walk of length one starting at any point, in any cycle graph. But of course this is true because our definition of a cycle graph stipulates that the point $p_{n-1}$ has only a single edge going to point $p_0$, while all other points $p_k$ only have a single edge going to point $p_{k+1}$. As such, a walk of length one is possible from any point; furthermore, there is only one such walk since there is only a single edge that can be traversed.\\
\\
\textit{Inductive Hypothesis}\\
Let us assume that for all $1 \leq j < k$, for all cycle graphs, that it is the case that there is only a single walk that can be taken of length $j$ starting from any point within the graph. In other words, that $\omega(p_i, j)$ is defined on all points of all cycle graphs whenever $j < k$.\\
\\
\textit{Inductive Step}\\
We must now show that we can complete a walk of length $k$ starting from any point in any cycle graph, and that there is only one walk that we can take to do so.\\
\\
Our inductive hypothesis tells us that there is only a single walk of length $k - 1$ we can take starting from any point $p_i$, ending at point $\omega(p_i, k - 1)$.\\
\\
Furthermore, our base case tells us that there is only a single walk of length one that can be taken from any point in our graph. As such, it must be the case that there is only a single walk of length one that can be taken from point $\omega(p_i, k - 1)$; equivalently we know $\omega(\omega(p_i, k - 1), 1)$ is defined.\\
\\
Since our composite walk $\omega(\omega(p_i, k - 1), 1)$ is composed of a walk of length $k - 1$ and a second walk of length $1$, our composite walk has length $k$. Furthermore, since both components of our walk were the only walks that could have been taken, we can conclude that our composite walk is the only walk that could have been taken of length $k$. As such, not only have we completed our proof, but we have also shown that: 
\begin{center}
$\omega(p_i, k) = \omega(\omega(p_i,$ $k - 1),$ $1)$ whenever $k > 1$
\end{center}
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center} 
\noindent
\\
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any point $p_i$, for any $0 \leq t \leq j,$:
\begin{center}
$\omega(p_i, j)$ $=$ $\omega(\omega(p_i, t), j - t)$ $=$ $\omega(\omega(p_i, j - t), t)$. 
\end{center}
\end{tcolorbox}
\noindent
\textit{Proof}\\
\\
We know there exists a single walk of length $t$ starting at point $p_i$.
Let $p_w$ be the point this walk ends at, then $p_w = \omega(p_i, t)$.\\
\\
We know there exists a single walk of length $j - t$ starting from point $p_w$.
Let $p_y$ be the point this walk ends at, then $p_y = \omega(p_w, j - t)$.\\
Applying substitution yields:
\begin{center}
$p_y = \omega(\omega(p_i, t), j - t)$
\end{center}
Combining the two above walks yields a walk from $p_i$ to $p_y$ of length\\
$t + (j - t) = j$. Thus, we have shown:
\begin{center}
$p_y = \omega(p_i, j) = \omega(\omega(p_i, t), j - t)$
\end{center}
To complete our proof, we need to show:
\begin{center}
$\omega(p_i, j) = \omega(\omega(p_i, j - t), t)$.
\end{center}
Let us define $a = j - t$.\\
From the first part of this proof, we know that
\begin{center}
$\omega(p_i, j) = \omega(\omega(p_i, a), j - a)$
\end{center}
Substituting out $a$ yields:
\begin{center}
$\omega(p_i, j) = \omega(\omega(p_i, j - t), j - (j - t)) = \omega(\omega(p_i, j - t), t)$.
\end{center}
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\begin{tcolorbox}
\textbf{Corollary}\\
For any cycle graph $G_n$, for any point $p_i$, for any non-negative integers $t, v$:
\begin{center}
$\omega(p_i, t + v) = \omega(\omega(p_i, t), v) = \omega(p_i, v), t)$
\end{center}
\end{tcolorbox}
\noindent
\textit{Proof}\\
Let $j = t + v$.
Through the previous lemma, we know that
\begin{center}
$\omega(p_i, t + v) = \omega(p_i, j) = \omega(\omega(p_i, t), j - t)$\\
$\omega(p_i, t + v) = \omega(p_i, j) = \omega(\omega(p_i, j - t), t)$
\end{center}
Substitution brings us:
\begin{center}
$\omega(p_i, t + v) = \omega(\omega(p_i, t), (t + v) - t) = \omega(\omega(p_i, t), v)$
$\omega(p_i, t + v) = \omega(\omega(p_i, (t + v) - t), t) = \omega(\omega(p_i, v), t)$
\end{center}
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any integers $i, j$ such that
\begin{center}
$0 \leq j \leq i < n$
\end{center}
then
\begin{center}
$\omega(p_j, i - j) = p_i$
\end{center}
\end{tcolorbox}
\noindent
\textit{Proof}\\
This will be a proof by induction on the length of the walk $l = i - j$.\\
\\
\textit{Base Case} ($length = 0$)\\
Let $l = 0$ be the length of the walk, giving us $l = i - j = 0$ and $i = j$.\\
Substituting gives us:
\begin{center}
$\omega(p_j, l) = \omega(p_j, i - j) = \omega(p_j, 0) = p_j = p_i$
\end{center} 
\noindent
\\
\textit{Hypothesis}\\
Assume for all $l < t \leq n - l$, that $\omega(p_j, l) = \omega(p_j, i - j) = p_i$\\
\\
\textit{Inductive Step}\\
We need to show that $\omega(p_j, t) = \omega(p_j, i - j) = p_i$.\\
\\
By a previous lemma, we know that:
\begin{center}
$\omega(p_j, t) = \omega(\omega(p_j, t - 1), 1)$
\end{center}
Focusing on the inner function and substituting:
\begin{center}
$\omega(p_j, t - 1) = \omega(p_j, (i - j) - 1) = \omega(p_j, (i - 1) - j)$
\end{center}
Applying the inductive hypothesis:
\begin{center}
$\omega(p_j, t - 1) = \omega(p_j, (i - 1) - j) = p_{i-1}$
\end{center}
Substituting $p_{i-1}$ back into $\omega(p_j, t)$ yields:
\begin{center}
$\omega(p_j, t) = \omega(\omega(p_j, t - 1), 1) = \omega(p_{i-1}, 1) = p_i$
\end{center}

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\begin{tcolorbox}
\textbf{Corollary}\\
For any cycle graph $G_n$, for any integers $i, j$ such that
\begin{center}
$0 \leq j < i < n$
\end{center}
the shortest walk from $p_j$ to $p_i$ has length $i - j$.
\end{tcolorbox}
\noindent
\textit{Proof}\\
Our lemma proved that $\omega(p_j, i - j) = p_i$, for $0 \leq j \leq i < n$.
As such, we need to show that for any $l < i - j$, that $\omega(p_j, l) \neq p_i$.\\
\\
Let $1 \leq l < i - j$. Let $x$ be the integer such that $l = (i - j) - x$.
Then,
\begin{center}
$\omega(p_j, l) = \omega(p_j, (i - j) - x) = \omega(p_j, (i - x) - j)$ 
\end{center}
Our lemma tells us that whenever $j < i - x$, then $\omega(p_j, (i - x) - j) = p_{i-x}$.\\
From $l = (i - j) - x$, we get $x = (i - j) - l$. Substituting,
\begin{center}
$i - x = i - ((i - j) - l) = i - i + j + l = j + l$
\end{center}
By stipulation, we have $1 \leq l$. Therefore, $j < j + 1 \leq j + l = i - x$.\\
\\
With our inequality shown, we can state $\omega(p_j, l) = p_{i-x}$ for all possible $l$.\\
\\
If we can show it is always the case that $x > 0$, then $p_{i-x} \neq p_i$, and we will have completed our proof.\\
\\
By stipulation, we know that
\begin{center}
$l = (i - j) - x < i - j$
\end{center}
We can subtract $(i - j)$ to get $-x < 0$.\\
\\
Finally, we can multiply by $-1$, yielding $x > 0$.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any integers $i, j$ such that
\begin{center}
$0 \leq i \leq j < n - 1$
\end{center}
then
\begin{center}
$\omega(p_j, (n - j) + i) = p_i$
\end{center}
\end{tcolorbox}
\noindent
\textit{Proof}\\
We shall prove this directly. Using the previous lemma, we know that:
\begin{center}
$\omega(p_j, (n - 1) - j) = p_{n-1}$ since $j \leq (n - 1)$
\end{center}
Furthermore, we know that:
\begin{center}
$\omega(p_{n-1}, 1) = p_0$ by structure of $G_n$\\
$\omega(p_0, i) = \omega(p_0, i - 0) = p_i$ since $0 \leq i$
\end{center}
As such, we can create a composite of these three walks to form a walk from $p_j$ to $p_i$ having length:
\begin{center}
$((n - 1) - j) + 1 + i = (n - j) + i$
\end{center}
which completes our proof.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\begin{tcolorbox}
\textbf{Corollary}\\
For any cycle graph $G_n$, for any integers $i, j$ such that
\begin{center}
$0 \leq i \leq j \leq n - 1$
\end{center}
the shortest walk from $p_j$ to $p_i$ has length $(n - j) + i$.
\end{tcolorbox}
\noindent
\textit{Proof}\\
We will complete this proof by showing that $\omega(p_j, l) \neq p_i$ for all $l < (n - j) + i$.\\
\\
We will start by assuming we have $l \leq (n - j)$. For any such $l$, we can find some $k$ such that $l = (n - j) - k$ where $0 \leq k < n - j$. Substituting yields:
\begin{center}
$\omega(p_j, l) = \omega(p_j, (n - j) - k) = \omega(p_j, (n - k) - j)$
\end{center}
By a previous lemma, we know that $\omega(p_j, (n - k) - j) = p_{n - k}$ as long as $n - k > j$.\\
\\
We know that $k < n - j$ by our selection of $l$. Adding $j$ yields $k + j < n$, followed by subtracting $k$ give us $j < n - k$. Thus, we are able to apply our lemma to give us $\omega(p_j, (n - k) - j) = p_{n - k}$. Furthermore, we know $i \leq j < n - k$, thus we have that $i \neq n - k$, yielding:
\begin{center}
$\omega(p_j, l) = \omega(p_j, (n - k) - j) = p_{n-k} \neq p_i$.
\end{center}
As such, we have eliminated all $l \leq (n - j)$.\\
\\
We now need to show that if $l$ is within: $n - i < l < (n-1) + i$, then $\omega(p_j, l) \neq p_i$.\\
\\
For any $l$, we can find some $k$ such that $l = (n - j) + k$ where $1 \leq k < i$. Substituting allows us to apply the previous lemma:
\begin{center}
$\omega(p_j, l) = \omega(p_j, (n - j) + k) = p_k \neq p_i$.
\end{center}

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\begin{tcolorbox}
\textbf{Corollary}\\
For any cycle graph $G_n$, for any points $p_i, p_j$, there exists positive integers $a, b \leq n$ such that:
\begin{center}
$\omega(p_i, a) = p_j$ and
$\omega(p_j, b) = p_i$.
\end{center}
In other words, it is always possible to walk from any point $p_i$ to any point $p_j$ with a length no greater than $n$.\\
\\
Furthermore, whenever $p_j \neq p_i$, then the integers $a, b$ will satisfy the relationship: $a = n - b$ (equivalently $b = n - a$).\\
\\
Finally, whenever $p_j = p_i$, then we will have $a = b = n$.
\end{tcolorbox}
\noindent
\textit{Proof}\\
The first part of this proof follows directly from the previous two lemmas.\\
\\
Let $p_i, p_j$ be any two points such that $i < j$. We know that $\omega(p_i, j - i) = p_j$ by the first of our two previous lemmas. Furthermore, $j - i < j < n$.\\
\\
Now assume that $j \leq i$. We know that $\omega(p_i, (n - i) + j) = p_j$ by the second of our two previous lemmas. Furthermore, $(n - i) + j \leq (n - i) + i = n$.\\
\\
As such, we completed showing that from any point $p_i$, there exists a walk to any other point $p_j$ with a length less than or equal to $n$.\\
\\
We will now prove that whenever $p_j \neq p_i$, that $b = n - a$.\\
\\
Assume that $i < j$. We know that $\omega(p_i, j - i) = p_j$, and $\omega(p_j, (n - j) + i) = p_i$. Therefor, it must be the case that
\begin{center}
$a = j - i$\\
$b = (n - j) + i = n - (j - i) = n - a$
\end{center}
We will conclude this proof by showing that a walk of length $n$ from any point $p_i$ will return to point $p_i$.\\
\\
By the previous lemma, we know a walk of length $(n - i) + i) = n$ will end on point $p_i$ since $i \leq i$.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any point $p_i$, for any integers $a, t \geq 0$
\begin{center}
$\omega(p_i, t) = \omega(p_i, t + an)$
\end{center}
\end{tcolorbox}
\noindent
\textit{Proof}\\
TODO
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\begin{tcolorbox}
\textbf{Theorem}\\
For any cycle graph $G_n$, for any point $p_i$, for any integer $t \geq 0$
\begin{center}
$\omega(p_i, t) = \omega(p_i, t \triangleleft n)$
\end{center}
where $t \triangleleft n $ is the remainder of $t$ when divided by $n$.
\end{tcolorbox}
\noindent
\textit{Proof}\\
TODO
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any point $p_i$, there is only a single walk of length $k$ that ends at point $p_i$. Stated using the walk function, we have: 
\begin{center}
$\omega(p_i, k) = \omega(p_j, k)$ if and only if $p_i = p_j$
\end{center}
\end{tcolorbox}
\noindent
\textit{Proof}\\
We shall again employ induction on the length of our walk in order to complete this proof.\\
\\
\textit{Base Case}\\ 
Our base case requires us to show that there is only a single walk of length one that ends at any point $p_k$.\\
\\
Let us begin by examing the case where our walk ends at the initial point $p_0$. We already know that $\omega(p_{n-1}, 1) = p_0$ by definition of our cycle graph. As such, all we have to do is show that $p_{n-1}$ is the only such point in our graph to have a walk end at point $p_0$. But we already know this to be true since $\Omega(p_k, 1) = k + 1 > 0$ for all $k < n - 1$.\\
\\
To complete our base case, we need to show that whevener $\Omega(p_i, 1) = k$ for some $k > 0$, that $p_i$ must be the only such point with this property. By the definition of a cycle graph, we know that point $p_{k-1}$ has only a single edge which goes to point $p_k$, thus we know $\omega(p_{k-1}, 1) = p_k$. As such, we must show that whenever $l \neq k - 1$, that it is also the case that $\Omega(p_l, 1) \neq k$.\\
\\
Assume $l < k - 1$, then by definition of a cycle graph:
\begin{center}
$\Omega(p_l, 1) = l + 1 \leq k - 1 < k$
\end{center}
As such, we have eliminated all points with an index less then $k - 1$ from consideration. Now we only need to show that if $k - 1 < l$, that $\Omega(p_l, 1) \neq k$. We already know that $\Omega(p_{n-1}, 1) = 0 < k$, thus leaving us with the cases where $l < n - 1$. For these final cases, our definition tells us:
\begin{center}
$\Omega(p_l, 1) = l + 1 > l \geq k$
\end{center}
Therefore, it must be the case that $p_{k-1}$ is the only point with the property that $\omega(p_{k-1}, 1) = p_k$ for $k > 0$. In doing so, we have completed our base case.\\
\\
\textit{Inductive Hypothesis}\\
Assume that for any $1 < j < k$, and any point $p_i$, that there is only a single walk of length $j$ that ends at point $p_i$.\\
\\
\textit{Inductive Step}\\
To complete out proof, we need to show that there is only a single walk of length $k$ that ends at point $p_i$.\\
\\
Our base case stipulates that there is only a single walk of length $1$, starting from some point $p_j$, which ends at point $p_i$. As such, in order for a walk of length $k$ to end at point $p_i$, the subwalk of length $k - 1$ will have to end at point $p_j$.\\
\\
Our inductive hypothesis tells us that there is only a single walk of length $k - 1$ that ends at point $p_j$. Thus, there is only one walk of length $k$ that ends at point $p_i$.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center} 
\noindent
Now that we know that our walks starting from any point of any length are not only valid, but also unique with regard to the starting point and length, we shall turn our attention to the predicting which point our walks will end on. To start, we shall examine the shortest walk between any two point $p_j$ and $p_k$ where $j < k$.\\
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any $j < k < n$, the shortest possible walk from point $p_j$ to $p_k$ has length $k - j$.\\
\\
Expressed via the walk function, we have that:
\begin{center}
$\omega(p_j, k - j) = p_k$ whenever $j < k$
\end{center}
Furthermore,
\begin{center}
$\omega(p_j, l) \neq p_k$ whenever $l < k - j$
\end{center}
\end{tcolorbox}
\noindent
\textit{Proof}\\


We shall note a few things about cycle graphs before we begin. First, for any point $p_j \in G_n$, there is only one edge that can be traversed. As such, there is only a single walk that can be made from $p_j$ of length $k$, for any $k$. Secondly, since we've labeled our points such that point $t$'s only edge goes to point $t + 1$ for $t < n - 1$; then for any $j < k < n$, we can think of a walk from $p_j$ to $p_k$ as a walk down the number line, and as such, the shortest walk from point $p_j$ to $p_k$ will be of length $k - j$.\\
\\

Before we get swept up into the theory of cycle graphs, let us first note some properties of our newly defined functions, $\omega$ and $\Omega$\\
\\
First off, we acknowledge that $\omega$ is a valid mapping there is only a single walk of length $k$, for any $k$, that we can take starting at any point since each of our points only have a single edge they can traverse.\\
\\
By definition of a cycle graph, point $p_{n-1}$ only has one edge going to point $p_0$. Furthermore, for any other point $p_k$ there is only a single edge going to point $p_{k+1}$. Thus, we have:
\begin{center}
$\omega(p_{n-1}, 1) = p_0$ and $\Omega(p_{n-1}, 1) = 0$\\
$\omega(p_k, 1) = p_{k+1}$ and $\Omega(p_k, 1) = k + 1$ for any $k < n - 1$\\
\end{center}
\noindent
Furthermore, we know that if we want to take a walk of length $k$, for any $k > 1$, starting from any point, $p_i$, then this can be accomplished by first taking a walk of length $g$, where $1 \leq g < k$, starting at point $p_i$ to arrive at some point $p_j$; and then we can extend our walk by taking a second walk of length $k - g$ starting at point $p_j$. Applying this to the walk function, we get:
\begin{center}
$\omega(p_i, k) = \omega(\omega(p_i, j),$ $k - j)$ for any $0 < j < k$.
\end{center}
Now we are ready to dive into the theory of cycle graphs. We shall begin our study by examing the shortest walk to the point $p_0$ from any point within the cycle graph.\\

\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$ and for any point $p_i$ within that graph, a walk of length $n - i$ will end at point $p_0$. Furthermore, this the shortest walk between any point $p_i$ and $p_0$.\\
\\
Applying the lemma to the walk function, we get: $\omega(p_i, n - i) = p_0$ for every point $p_i$; furthermore, $\omega(p_i, j) \neq p_0$ whenever $j < n - i$.
\end{tcolorbox}
\noindent
\textit{Proof}\\
We shall break our proof into two cases. We shall first consider the walk from the last point $p_{n-1}$ to the initial point $p_0$. Thus, our second case shall consider the walk to the initial point for all other points in our graph.\\
\\
\textit{Case 1}


First, let us consider the trivial cases where $j = n - 1$. By definition of a cycle graph, the point $p_{n-1}$ has only one edge going to point $p_0$, therefor, our walk has length $1 = (n - n) + 1 = n - (n - 1)$. Furthermore, a walk of length $1$ is the shortest possible walk of any graph, thus our walk from $p_{n-1}$ to $p_0$ must be the shortest walk possible between these two points.\\
\\ 
Now, let us consider the walk from $p_j$ to the point $p_{n-1}$ for $j < n - 1$.\\
\\
As we noted above, the shortest walk between any two points $a < b < n$ will be of length $b - a$ by virtue of our labeling of the points of a cycle graph. As such, the shortest walk from $p_j$ to $p_{n-1}$ will have length $(n - 1) - j$.\\
\\
By definition of a cycle graph, we know that point $p_{n-1}$'s only edge goes to point $p_0$. As such, if we extend our walk from point $p_j$ to $p_{n-1}$ one step further, we will have arrived at point $p_0$, thus giving us a length of $((n - 1) - j) + 1 = n - j$.\\
\\
Furthermore, our walk from $p_j$ to $p_{n-1}$ was the shortest possible walk from $p_j$ and $p_{n-1}$; and our extension of that walk from $p_{n-1}$ to $p_0$ was also the shortest possible walk from $p_{n-1}$ to $p_0$. As such, our composite walk from $p_j$ to $p_0$ must be the shortest possbile walk from $p_j$ to $p_0$.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center} 
\noindent
We shall take a moment to note that as a direct result of the above lemma, the shortest walk from point $p_0$ back to point $p_0$ will have length $n - 0 = n$. This matches our intuition nicely since a walk from $p_0$ back to itself is similar to a lap around a race track, and as such will have to visit every point in our cycle graph, and we have $n$ total points.\\
\\
In our next lemma, we will see that a walk of length $n$ from any point in our graph will return us back to the point whence our walk originated. This should be fairly obvious since for any point $p_j$, we could relabel our graph to make $p_j$ our initial point $p_0$, and we already know that the shortest walk from $p_0$ back to itself has length $n$.\\
\\
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $CG_n$, a walk of length $n$ from $p_i$ will end at point $p_i$. Furthermore, this is the shortest possible walk from any point back to itself.
\end{tcolorbox}
\noindent
\textit{Proof}\\
As we saw in the previous lemma, for any point $p_i \in CG_n$, a walk of length $n - i$ is the shortest possible walk from $p_i$ to point $p_0$.\\
\\
Furthermore, we also know that the shortest walk from $p_0$ to any point $p_j$ for $j > 0$ will have length $j - 0 = j$.\\
\\
Combining these two facts together, we can take a walk from $p_i$ back to $p_i$ by first taking a walk to $p_0$ in $n - i$ steps, and then continuing our walk from $p_0$ to $p_i$ in $i$ more steps, giving us a total walk length of $(n - i) + i = n$.\\
\\
Furthermore, our constructed walk is composed of two shortest possible walks, and is therefor the shortest possible walk from $p_i$ back to itself.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center} 

Now that we know we can return to point $p_i$ by taking a walk of length $n$ around our graph, we can easily extend this idea to show that a walk of length $kn$ for $k > 0$ will also return us to point $p_i$. As such, a walk of length $kn$ is equivelant to taking $k$ laps around our cycle graph.\\ 
\begin{tcolorbox}
\textbf{Corollary}\\
For any cycle graph $CG_n$, a walk of length $kn$, $k > 0$, from $p_i$ will end at point $p_i$.
\end{tcolorbox}
\noindent
\textit{Proof}\\
Let us prove this corollary via induction on $k$, the number of laps around our graph.\\
\\
The base case, $k=1$, is the result of our previous lemma, in particular, that a walk of length $n = 1n=$ will return us back to point $p_i$.\\
\\
Now, let us assume that for all $j < k$, that a walk of length $jn$ from point $p_i$ will return us to point $p_i$.\\
\\
By assumption, we can take a walk of length $jn$ starting at point $p_i$ to arrive back at point $p_i$. As such, if we extend this walk by $n$ more steps, we will again return to $p_i$ by the above lemma; thus giving us a total walk of length $jn +n = (j+1)n$.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center} 

With the proof of the above lemma, we now have enough basic knowledge about cycle graphs in order to reason about the result of any walk on our graph. Furthermore, we shall see that our cycle graphs, and the walks we take around them, are merely another way of thinking about modular arithmetic.\\

Before we state our main theorem for cycle graphs, let us get some notation out of the way. For non-zero integer, $n$, and any integer $j > 0$, we write $j$ $mod$ $n = k$ (read "$j$ modulo $n$ is $k$") to imply that $j = mn + k$ where $0 \leq k < n$, for some integer $m$.\\
\begin{tcolorbox}
\textbf{Theorem}\\
For any cycle graph $C_n$, for any $k > 0$, for any $p_i \in C_n$, a walk of length $k$ starting at $p_i$ will end at point $p_j$, where $j = (i + k)$ $mod$ $n$.\\
\end{tcolorbox}
\noindent
\textit{Proof}\\
To began our proof, let $k > 0$ be the length of our walk of arbitrary length around our cycle graph, $CG_n$, starting from any point $p_i$.\\
\\
Rather than trying to prove that the statement is true for arbitrary $k$, we shall instead break our proof into four seperate cases:
\begin{enumerate}
\item When our walk is short enough to not take us past point $p_{n-1}$: $k < n -i$
\item When our walk ends at point $p_0$: $k = n - i$
\item When our walk takes us past point $p_0$, but is no longer than the number of points in our graph: $n - i < k \leq n$
\item When our walk is longer than the number of points in our graph: $n < k$
\end{enumerate}
For each of these cases, we shall show that we will end our walk on some point, $p_j$, such that $0 \leq j < n$; and $i + k = j + xn$, for some integer $x$. In doing so, we will have proven that $j = (i +k)$ $mod$ $n$ by the definition of modular arithmetic.\\
\\ 
\textit{Case 1}\\
Let us first consider the case where our walk is short enough to not take us past point $p_{n-1}$, in other words, let us consider the case where $k < n - i$.\\
\\
In this case, we know our walk will end at some point $p_j$, where $j = i + k$. Since we have $k < n - i$, we also know that $0 < i + k = j < n$. Furthermore, $i + k = j = j + 0n$, and as such, $j = (i + k)$ $mod$ $n$.\\
\\
\textit{Case 2}\\
Now let us consider the simple case where $k = n - i$. By a previous lemma, we know that a walk of lenth $n - i$ will end on point $p_0$ if started from point $p_i$. Furthermore, $i + k = i + (n - i) = n = 1n + 0$, thus, $0 = (i + k)$ $mod$ $n$.\\
\\
\textit{Case 3}\\
Now let us consider a slightly longer walk, but is still no longer than the number of points in our graph. In other words, let us consider a walk of length $n - i < k \leq n$.\\
\\
We know that the first $n - i$ steps of our walk will end at point $p_0$, leaving us with a remaining of $j = k - (n - i)$ steps in our walk. Furthermore, we have:
\begin{center}
$(n - i) - (n - i) = 0 < k - (n - i) = j \leq n - (n - i) = i < n$ 
\end{center}
Since $j < n$, we know that extending our walk $j$ more steps from point $p_0$ will end on point $p_j$ to complete our walk of length $k$. Not only that, but we also have
\begin{center}
$i + k = k + i + (n - n) = k - n + i + n = k - (n - i) + n = j + n$
\end{center} 
Thus, we know that $j = (i + k)$ $mod$ $n$.\\
\\
\textit{Case 4}\\
Now let us turn to the final case we must consider, the case where our walk length is greater than the number of points in our graph; in other words, let $k > n$. To complete our proof, we shall show that we can reduce this final case to one of our previous three cases.\\
\\
Since $k$ is a positive integer, we can find a positive another integer $a$ such that $an < k \leq (a + 1)n$. As such, by our previous corollary, we know that we can walk the first $an$ steps around our graph to return us to point $p_i$, and leave us with a remainder of $l = k - an$ steps in our walk of length $k$.\\
\\
Since we have returned to point $p_i$ with $0 < l \leq n$ steps remaining in our walk, we have now reduced our problem to one of our three previous cases. For each of the cases, we have:
\begin{enumerate}
\item If $l < n - i$, then we know we will end our walk on point $p_j$ where $j = l + i < n$. Thus, we have that $i + k = i + l + an = j + an$.
\item If $l = n - i$, then we know we will end our walk on point $p_0$. Thus, we have that $i + k = i + l + an = i + (n - i) + an = (a + 1)n + 0$.
\item Finally, if $n - i < l \leq n$, then we know we will end our walk at point $p_j$ where $j = l - (n - i)$. Thus, we have that $i + k = i + l + an = l + i - n + (a + 1)n = l - (n - i) + (a + 1)n = j + (a + 1)n$.
\end{enumerate}
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\section{Activation Cycle Machines}
With a few, minor modifications to cycle graphs, we can create \textit{activation cycle machines}; and in doing so, bring ourselves one step closer to understanding awkward state machines. So what exactly is an activation cycle machine? Let us answer that question by first looking at the formal definition; and then taking some time to examine its meaning afterward.\\
\\
\begin{tcolorbox}
\textbf{Definition}\\
An \textit{activation cycle machine} is a state machine with $n > 1$ states, $s_0, s_1, ..., s_{n-1}$, such that for any state, $s_k$ where $k < n - 1$, state $s_k$ has only a single transition to state $s_{k+1}$; and state $s_{n-1}$ has only a single transition to $s_0$. Furthermore, all activation cycle machines always start at state $s_0$.\\
\\
The first $a$ states, where $1 \leq a < n$, of an activation cycle machine are called \textit{activators}. An activation cycle machine is said to be \textit{active} when it's current state is one of it's activators, and is called \textit{inactive} when it's not active. We write $C_{n,a}$ to refer to the activation cycle machine with $n$ states and $a$ activators.\\
\\
When an activation cycle machine's current state is $s_k$, for any $k$, we call $k$ its \textit{position}, and write $\overline{C}(j)$ to refer to the position of the machine after it's $j^{th}$ transition. 
\end{tcolorbox}
\noindent
\\
The first part of our definition is almost exactly the same as our definition for a cycle graphs. However, instead of talking about points in a graph, and the edges between those points; we define the states of a machine, and the transitions between those states. With such similar definitions, it seems reasonable that we can represent the states of an activation cycle machine by using a cycle graph. In fact, we'll begin our study of activation cycle machines by proving just that.\\
\\
The definition of an activation cycle machine extends a bit beyond that of cycle graphs by incorporating the notion of \textit{activators},  which is merely a label applied to the first $a$ states of the machine, $s_0, ..., s_{a-1}$. Furthermore, our definition stipulates that while there is always at least one activator, there are never as many activators as there are states in the machine. As such, every activation cycle machine will become \textit{active} at least once, as well as \textit{inactive} at least once as it transitions between its states.\\
\\
Before we dive into our study of activation cycle machines, below you will find two working examples of an activation cycle machine coded in \textit{ruby}. The first example is more condensed. Rather than keep track of states directly, the first example simulates an activation cycle machine by keeping track of the current position of the machine using arithmetic. Our second example, on the other hand, is represented using states, and as such, needn't rely on arithmetic to function. Whether or not you program, I encourage you to look through both examples carefully; I think you will find they provide a bit of color to our abstract definition.
\begin{tcolorbox}
\begin{verbatim}
class ActivationCycleMachine
  def initialize(number_of_states, number_of_activators)    
    unless number_of_states > 1
      raise 'There must be more than one state'
    end
    
    unless number_of_activators.positive?
      raise 'The number of activators must be positive'
    end
    
    unless number_of_activators < number_of_states
      raise 'The number of activators must be less ' \
            'than the number of states'
    end
    
    @number_of_states = number_of_states
    @number_of_activators = number_of_activators
    @position = 0
  end

  def next_state
    if @position == @number_of_states - 1
      @position = 0
    else
      @position = @position + 1
    end
  end
  
  def active?
    @position < @number_of_activators
  end
  
  def inactive?
    !active?
  end
  
  def position
    @position
  end
end
\end{verbatim}
\end{tcolorbox}
\noindent
\\
As you can see, our first example simply increments the position of our state machine by one to simulate a transition, with the exception that it resets the position to 0 once its gotten to the maximum position. Since this example does not use states, it does not have a direct understandg of an \textit{activator}. Despite that, the implementation is able to determine activeness indirectly by checking if it's current position is less than the number of activators.\\
\\
We'll need to break our second example into two seperate classes. Within the first class, we will represent a single state in our machine, which will be implemented as a node in a linked list would. Our second class will be the representation of the activation cycle machine, which we implement much the same way that we would a linked list.\\
\\
\begin{tcolorbox}
\begin{verbatim}
class State
  def initialize(is_activator, position)
    @is_activator = is_activator
    @position = position
  end

  def activator?
    @is_activator
  end
  
  def next_state=(next_state)
  	@next_state = next_state
  end
  
  def next_state
  	@next_state
  end
  
  def position
  	@position
  end
end
\end{verbatim}
\end{tcolorbox}
\begin{tcolorbox}
\begin{verbatim}
class ActivationCycleMachine
  def initialize(number_of_states, number_of_activators)
    unless number_of_states > 1
      raise 'There must be more than one state'
    end

    unless number_of_activators.positive?
      raise 'The number of activators must be positive'
    end

    unless number_of_activators < number_of_states
      raise 'The number of activators must be less ' \
            'than the number of states'
    end

    initial_state = State.new(true, 0)
    current_state = initial_state
    (1...number_of_states).each do |i|
      activator = i < number_of_activators
      current_state.next_state = State.new(activator, i)
      current_state = current_state.next_state
    end
    current_state.next_state = initial_state

    @current_state = initial_state
  end

  def next_state
    @current_state = @current_state.next_state
  end

  def active?
    @current_state.activator?
  end

  def position
    @current_state.position
  end
end
\end{verbatim}
\end{tcolorbox}
\noindent
\\
As you can see, our second example matches our formal mathematical definition very closely. In particular, the activation cycle machine, once initialized, only has a notion of a current state. The machine is operated by simply replacing the current state with its next state, thus simulating a transition. Furthermore, since the states themselves encode whether or not its an activator; the activation cycle machine is able to determine if its active by simply asking if its current state is an activator; matching our mathematical definition exactly.\\
\\
Without further ado, let us now tie activation cycle machines and cycle graphs together.\\
\begin{tcolorbox}
\textbf{Lemma}\\
Let $C_{n,a}$ be any activation cycle machine.\\
Let $S = \{$ $s_i$ $|$ $0 \leq i < n$ $\}$ be the states of $C_{n,a}$.\\
\\
Let $G_n$ be the cycle graph of $n$ points.\\
Let $P = \{$ $p_j$ $|$ $0 \leq j < n$ $\}$ be the points of $G_n$.\\
\\
Let $\gamma: P \rightarrow S$ be the mapping from the points in our cycle graph to the states in our activation cycle machine such that $\gamma(p_k) = s_k$.\\
\\
Let $\theta: S \rightarrow P$ be the mapping from the states of our activation cycle machine to the points in our graph such that $\theta(s_k) = p_k$.\\
\\
Let $\omega: (P, \mathbb{N}) \rightarrow P$ such that $\omega(p_i, k) = p_j$, where $p_j$ is the point arrived to after a walk of length $k$.\\
\\
Let $\sigma: (S, \mathbb{N}) \rightarrow S$ such that $\sigma(s_i, k) = s_j$, where $j$ is the position of the machine after $k$ transistions when starting at state $s_i$.\\  
\\
Then $\gamma(\omega(p_i, 1)) = \sigma(s_i, 1)$.\\
\\
In other words, for any activation cycle machine, we can determine the next state to be transitioned to by instead taking a walk of length one, starting at the point with the same index as the starting state, around the cycle graph with the same number of points as states in our machine; and then finally mapping the resulting point from our walk back to the states in our machine using $\gamma$.
\end{tcolorbox}
\noindent
\textit{Proof}\\
Our proof is rather trivial and will come directly from our definitions. Let us assume we have some activation cycle machine with $n$ states, $S = \{s_0, s_1, ..., s_{n-1}\}$, and the cycle graph with $n$ points, $P = \{p_0, p_1, ..., p_{n-1}\}$. Let us define the mapping $\theta: S \rightarrow P$ as $\theta(s_k) = p_k$, and it's inverse $\gamma: P \rightarrow S$ which is clearly $\gamma(p_k) = s_k$.\\
\\
To begin, we will show that if our activation cycle machine is on any state, $s_k$, that we can determine the next state our machine will transition to by taking a walk of length $1$ starting at point $\theta(s_k) = p_k$ and mapping the resulting point, $p_j$, back to state $\gamma(p_j) = s_j$.\\
\\
Let our activation cycle machine be at position $n - 1$. If we were to take a walk of length $1$ from point $\theta(s_{n-1}) = p_{n-1}$, then we will arrive at point $p_0$ by definition of a cycle graph. Mapping point $p_0$ back to our cycle machine under $\gamma$ yields state $\gamma(p_0) = s_0$. By definition of an activation cycle machine, we know that state $s_{n-1}$ transitions to state $s_0$, thus our mappings hold for $n - 1$.\\
\\
Now let us assume our activation cycle machine is at position $k < n - 1$. If we were to take a walk of length $1$ from point $\theta(s_k) = p_k$, then we will arrive at point $p_{k+1}$ by definition of a cycle graph. Mapping point $p_{k+1}$ back to our cycle machine under $\gamma$ yields state $\gamma(p_{k+1}) = s_{k+1}$. By definition of an activation cycle machine, we know that state $s_k$ transitions to state $s_{k+1}$, thus our mappings hold for all $k$.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}
\noindent
\\
With the proof of this simple lemma, we can immediately apply all our knowledge of cycle graphs to activation cycle machines. Most importantly, we gain the ability to predict the position of any activation cycle machine after any number of transitions.\\
\begin{tcolorbox}
\textbf{Corollary}\\
For any $n$, for any $k$, the position of the activation cycle machine with $n$ states after $k$ transitions starting from any position $j$ can be determined by taking a walk of length $k$ starting from point $p_j$ around the cycle graph with $n$ points.
\end{tcolorbox}
\noindent
\textit{Proof}\\
Since we can predict the resulting state of a transition of our cycle machine by instead taking a walk of length $1$ on our corresponding cycle graph, it should be obvious that we can also predict the resulting state of our cycle machine after $k$ transitions by instead taking a walk of length $k$ on our cycle graph.\\
\\
We shall prove our corollary by induction on $k$, the number of transitions our state machine undergoes. Our base case, $k = 1$, is the direct result of our previos lemma. As such, let us assume for all $j \leq k$, that the resulting position of our cycle machine after $j$ transitions from any position, $i$, can be determined by instead taking a walk of length $j$ starting at point $p_i$ on our cycle graph and mapping the resulting point back to our cycle machine under our mapping $\gamma: P \rightarrow S$, $\gamma(p_l) = s_l$.\\
\\
To transition $k + 1$ states, let us first transition $k$ states from our position, $i$. By assumption, we know that our resulting state will be given by taking a walk from point $p_i$ of length $k$ and mapping the resulting point, $p_l$, back to our machine under $\gamma$, giving us state $\gamma(p_l) = s_l$. We now only have one transition left to complete our $k + 1$ transitions. By our lemma, we know this final transition from our state $s_l$ is given by taking extending our walk one more step from $p_l$ and mapping the result back to our machine. As such, mapping the result of a walk of length $k + 1$ around our cycle graph does in fact result in the same position as $k + 1$ transitions would have.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}
\noindent
\\
\begin{tcolorbox}
\textbf{Corollary}\\
For any $n$, for any $k$, the position of the activation cycle machine with $n$ states after $k$ transitions starting from any position $j$ is given by $(j + k)$ $mod$ $n$.
\end{tcolorbox}
\noindent
\textit{Proof}\\
By our previous corollary, we know the position of our cycle machine after $k$ transitions starting at any position, $j$, can be determined by instead taking a walk of length $k$ starting from point $p_j$ around the cycle graph with $n$ points.\\
\\
By our previous theorem on cycle graphs, we know our walk of length $k$ will end at point $p_i$, where $i = (j + k)$ $mod$ $n$. As such, the position of our cycle graph after $k$ transitions will also be $i$.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}
\noindent
\begin{tcolorbox}
\textbf{Lemma}\\
For any activation cycle machine $C_{n,a}$, for any $k$, the activation cycle machine is active if and only if $a > (k$ $mod$ $n)$.
\end{tcolorbox}
\noindent
\textit{Proof}\\
By our previous corollary, we know that the position of our cycle machine, $C_{n,a}$, after $k$ transitions will be given by $j =(k + i)$ $mod$ $n$, where $i$ is the initial position of our machine. By definition of an activation cycle machine, we know that $i = 0$, thus, our position after the first $k$ transitions is given by $j = k$ $mod$ $n$.\\
\\
Furthermore, an activation cycle machine is defined to be active whenever its current state is one of its activators, in other words, whenever its position is less than $a$. Putting the two together, we get that our cycle machine is active after the initial $k$ transitions whenever $a > k$ $mod$ $n$
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}
  
\section{Awkward State Machines}

Now that we have our definitions for cycle graphs and ACMs, we are almost ready to formally define the \textit{awkward state machine} or \textit{ASM}. An ASM is a state machine for the purpose of generating a set of ACMs using a pre-existing set of ACMs. In order to accomplish this task, with each step of an ASM, all the pre-existing ACMs state's are incremented by one; if none of the ACMs are active after moving, then a new ACM is added to the ASM and it's position is set to 0. Thus, every state of an ASM has at least one ACM that is active.\\
\\
Every ASM starts with an initial ACM set to position 0, and an \textit{activation branch} of length one greater than the ACM. An activation branch is simply an ACM that has the edge removed from its last point to its inital point; thus an activation branch forms a line instead of a circle. Whenever the ASM must create a new ACM, it does so by creating a copy of its activation branch, then adds the missing edge to the original branch in order to form the new ACM required by the machine.  Furthermore, a new node is added to the end of the copied activation branch. If the ASM does not need to create a new ACM after moving, then a new node is added to the end of its activation branch. Thus, the length of an ASM's activation branch increases by one with every iteration.\\
\\
This section will explore several fundamental questions about ASMs and the sets of ACMs they generate. We will find that the sets of ACMs generated by ASMs have a unique and unexpected relationship to modular arithmetic. For instance, we will see that the most basic ASM generates the prime numbers; and all other ASMs generate sets of integers just as strange, or awkward, as the primes. Furthermore, in our final theorem of this section, we will show that every ASM will generate an infinite set of ACMs if left running.\\   
\\
We will begin this section by formally defining the activation branch and proving that we can indeed create an ACM by adding an edge to it. From there, we will formally define ASMs and then begin our exploration.\\
\\
\\
\textbf{Definition}\\
An \textit{activation branch} is directed graph that forms a line. Explicity, the activation branch of length $n$ has points $\{$ $p_0, p_1 ..., p_{n-1}$ $\}$ such that for every point $p_{i < n -1}$ has a single edge connecting it to $p_{i+1}$, and $p_{n-1}$ does not have any edges extending from it.\\
\\
Furthermore, an activation branch is equipped with $a > 0$ activator points: $A = \{$ $p_i$ $|$ $0 \leq i < a < n$ $\}$.\\ 
\\
We denote the activation branch with length $n$ and $a$ activator nodes $B_{n,a}$.
\\
\\
\textbf{Lemma}\\
For any $B_{n,a}$, adding an edge extending from $p_{n-1}$ to $p_0$ produces the cycle graph $CG_n$ for ACM $C_{n,a}$.\\
\\
\textit{Proof}\\ 
By definition of $B_{n,a}$, all points $p_{i < n -1}$ have the same edges as the first $n - 1$ points in $CG_n$ for ACM $C_{n,a}$.\\
\\
Furthermore, the first $a$ points of $B_{n,a}$ are it's activator points, which are the activators of $C_{n,a}$.\\
\\
Thus, the only edge missing from $B_{n,a}$ that is contained within $CG_n$ is from  $p_{n-1}$ to $p_0$.\\
\\
Therefore, adding the edge will convert $B_{n,a}$ into $CG_n$ for $C_{n,a}$.\\
\textit{Q.E.D}\\
\\
\textbf{Definition}\\
An \textit{awkward state machine} (ASM) is state machine running on top of a directed graph composed of a set of ACMs and a single activation branch.\\
\\
Every ASMs initial state contains a single $ACM$, $C_{m,a} = C_0$, and the activation branch, $B_{m+1,a}$.\\
Furthermore, the position of $C_0$ on the initial state is $0$.\\
We denote the ASM with an initial state containing $C_{m,a}$ as $S_{a,n=m-a}$.\\
\\
To progress from state $i - 1$ to state $i$ for ASM, $S$, first move every ACM in $S$ to it's next state.\\
If none of the ACMs in $S$ are active after moving to their next state, then:
\begin{enumerate}
\item Create a copy of the activation branch, giving you branches $B$ and $B`$.
\item Convert $B$ into an $ACM$, $C$, by adding an edge to it's last point, thus leaving a single activation branch $B`$. Set the position of $C$ to $0$. Now $C$ is contained within the set of ACMs for $S$.
\end{enumerate}
Regardless of whether one of the ACMs were active, add a new point to the end of the activation branch. Explicitly, if the activation branch had length $n$, then add an edge extending from $p_{n-1}$ to the new point $p_n$, thus creating an activation branch of length $n + 1$.\\
\\
If none of the ACMs were active after moving them to their next state, we say that $S$ is \textit{inactive} after moving; otherwise we say $S$ is \textit{active} after moving. Therefore, we only add a new ACM to $S$ when $S$ is inactive after moving.\\
\\
We denote the inition cycle of an ASM $C_0$, the first discovered cycle $C_1$, and the $n$th discovered cycle $C_n$.\\
\\
We denote the graph of ASM $S_{a,n}$ on state $k$ as $S^k$.\\
\\
If $C_{p,a}$ is discovered by ASM $S_{a,n}$ on some step $k$, then we say that $C_{p,a}$ is \textit{discoverable} by $S$.\\
\\
\textbf{Definition}\\  
We'll define $[S^a] = \{$ $C_i$ within the graph of $S$ on step $a$ $\}$,\\
and $[S] = \{$ $C_0$ and all $C_i$ disoverable by $S$ $\}$. We refer to $[S]$ as the \textit{school} of cycles for $S$.\\
\\
\\
\textbf{Lemma}\\
For $S_{a,n}$, the length of the branch, $B$, on step $k$ is given by $|B^k| = k + a + n + 1$.\\
\\
\textit{Proof}\\  
One step $0$, the length of branch $B$ is given by $|B^0| = 0 + a + n + 1$.\\
With each step, a single node is added to the branch $B$. Thus, $|B^{i+1}| = |B^{i}| + 1$.\\
\\
Assume $|B^{i}| = i + a + n + 1$.\\
Then $|B^{i+1}| = |B^{i}| + 1 = (i + a + n + 1) + 1 = (i + 1) + a + n + 1$.\\
\\
Thus we have shown that $|B^k| = k + a + n + 1$ by induction.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Lemma}\\
For $S_{a,n}$, if $C_i$ is discovered on step $k$, then the length of $|C_i| = |B^{k-1}| = k + a + n$.\\
\\
Assume for $S_{a,n}$, that $C_i$ is discovered on step $k$.\\
\\
To produce state $k$, the ASM algorithm first moved all $C_j$ for $j < i$ from state $C^{k-1}$ to state $C^k$. After doing so, there did not exist a $j < i$ such that $C_j$ was active. Therefor, the algorithm copied the branch $B^{k-1}$ and closed one of the two branches to create $C_i$.\\
\\ 
Thus, the length of $C_i$ is equal to the length of the branch $B^{k-1}$:\\ $|C_i| = |B^{k-1}| = (k - 1) + a + n + 1 = k + a + n$.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Lemma}\\
For $S_{a,n}$, $|C_{i}| \geq |C_{i-1}| + a$.\\
\\
\textit{Proof}\\  
Assume cycle $C_i$ is discovered by $S_{a,n}$ on step $k$.\\
\\
Then $\overline{C^k_i} = 0$.\\
Furthermore, the new branch, $B$, will have $|C_i| + 1$ nodes at step $k$.\\
\\
For next, $1 \leq j < a$ steps, the cycle  $C^{k+j}_i$ will be active since there are $a$ activation nodes.\\
Furthermore, the length of the branch $B$ at each step will be given by $|B| = |C_i| + j + 1$.\\
\\
The $a$th step after discovering $C_i$ will be the first time that $C_i$ will be inactive. The branch length of the $(a-1)$th step is given by $|B^{j + a - 1}| = |C_i| + a$. Thus, if $C_p$ for $0 \leq p < i$ are also inactive on step $a$th step after discovering $C_i$, then we will have to close $B$ on the $a$th step, thus creating cycle $C_{i+1}$ with length $|C_{i+1}| = |B^{j + a - 1}| = |C_i| + a$. Furthermore, if any of the cycles $C_p$ were active on the $a$th step after discovering $C_i$, then the branch would not close, thus the next cycle's length is at least as long as the branch on the $(j+a)$th step: $|C_{i+1}| >= |B^{j + a}| = |C_i| + a + 1$.\\
\\
Thus, we have shown that  $|C_{i}| \geq |C_{i-1}| + a$.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Lemma}\\
For any $C_i, C_j \in [S_{a,n}]$ with $j < i$, it holds that $|C_{i}| \geq |C_j| + (i - j)a$.\\
\\
\textit{Proof}\\ 
If $i = j + 1$, then $j - i = 1$.\\
Thus, $|C_i| \geq |C_j| + a = |C_j| + (j - i)a$.\\
\\
Assume for $k$, $i < k \leq j$, that $|C_k| \geq |C_i| + (k-i)a$.\\
\\
Then $|C_{k+1}| \geq |C_k| + a \geq (|C_i| + (k-i)a) + a = |C_i| + (k + 1 - i)a$.\\
\\
Thus we have shown that $|C_{i}| \geq |C_j| + (i - j)a$ by induction.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Corollary}\\
For any cycle $C_{j}$ of an ASM, $S_{a,n}$, $C_j$ has at least $n + ja$ non-activator nodes.\\
\\
\textit{Proof}\\
$C_0$ is defined to have $n$ activator nodes.\\
\\
Let $C_{j>0} \in [S_{a,n}]$.\\
\\
Then $|C_j| \geq |C_1| + (j-1)a \geq (|C_0| + a) + (j-1)a $\\
$= (2a + n) + (j-1)a = n + (j+1)a$.\\
\\
Thus, after removing the $a$ activator nodes from $C_j$, there are at least $n + ja$ non-activator nodes left.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Lemma}\\
Every ASM discovers at least one cycle.\\
\\
\textit{Proof}\\
Let $C_0$ by the initial cycle for ASM, $S_{a,n}$.\\
\\
After taking $a$ steps from the initial state of the $S$, the ASM will be on it's first non-activator node.\\
\\
Since $C_0$ is the only cycle, the ASM would be inactive, thus a new cycle would be created.\\
\textit{Q.E.D}\\
\\ 
\\
\textbf{Lemma}\\
For any step, $k$, for any $C_i \in [S^k_{a,n}]$, it holds that $\overline{C^k_i} = (k + a + n)$ $mod$ $|C_i|$.\\
\\
\textit{Proof}\\  
The initial cycle $C_0$ starts at position $0$ on step $0$. By properties of ACM's, we know that the position of $C_0$ is given by:\\
\\
$\overline{C^k_0} = k$ $mod$ $|C_0| $
$= [k +(a + n)]$ $mod$ $(a + n)$
$= (k + a + n)$ $mod$ $|C_0|$.\\
\\
If a cycle, $C_i$, is discovered on step $k$, then we know it's length is given by $|C_i| = k + a + n$. Furthermore, when it's dicovered, it's position is set to 0.\\
\\
Thus, the position of $C^k_i$ is given by:\\
\\
$\overline{C^k_i} = 0 = (k + a + n)$ $mod$ $(k + a + n) = (k + a + n)$ $mod$ $|C_i|$.\\
\\
By properties of ACM's, the position for all steps $j > k$ will be given by:\\
\\
$\overline{C^j} = (\overline{C^k} + (j - k))$ $mod$ $|C^i|$
$= (0 + (j-k))$ $mod$ $(k + a + n)$\\
$= (k + a +n) + (j-k)$ $mod$ $|C_i| $
$= (j + a + n)$ $mod$ $|C_i|$.\\
\\
Therefor we have shown that the position of $C_i$ on step $k$ for any $C_i \in [S^k_{a,n}]$ is given by $\overline{C^k_i} = (k + a + n)$ $mod$ $|C_i|$ for any $C_i$ in $[S^k]$.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Corollary}\\
For any cycle $C_j \in [S_{a,n}]$, moving $k|C_j|$ steps will maintain $C_j$'s position.\\
\\
\textit{Proof}\\
Let $C_j \in [S]$.\\
Assume $S$ is on step $p$.\\
\\
Then $\overline{C_j} = p + a + n $ $mod$ $|C_j|$.\\
Thus, if we move $k|C_j|$ steps from $p$, the position of $C_j$ will be given by:\\
\\
$p + a + n + k|C_j|$ $mod$ $|C_j| = p + a + n$ $mod$ $|C_j|$.\\
\\
Therefor $C_j$'s position was maintained.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Lemma}\\
For any cycle $C \in [S]$, if $k \geq |C|$, then the position of $C$ after moving $k$ steps from $C$'s discovery is the same as moving $k$ $mod$ $|C|$ steps.\\
\\
\textit{Proof}\\
Assume $C \in [S]$, and $S$ is on step $p$.\\
Then the position of $C$ is given by $p + a + n$ $mod$ $|C|$.\\
\\
Let $k > |C|$, $b = k$ $mod$ $|C|$.\\
\\
The position of $C$ on step $p + k$ is given by:\\
$p + k + a + n$ $mod$ $C = p + b + a + n$ $mod$ $C$ which is the position after moving $b$ steps from $p$.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Lemma}\\
For any $C_i, C_j \in [S_{a,n}]$ with $j < i$, it holds that $|C_i|$ $mod$ $|C_j| \geq a$.\\
\\
\textit{Proof}\\  
Assume for ASM, $S_{a,n}$, that ACM, $C_i$, is discovered on step $k$.\\
\\
Then $|C_i| = k + a + n$.\\
We know also know that the position of $C^k_j$ must be greater than or equal to $a$; otherwise, $C_j$ would be active on step $k$ and we would not have discovered $C_j$.\\
\\
Furthermore, we know the position of $C^k_j$ is given by:\\
$\overline{C^k_j} = (k + a + n)$ $mod$ $|C_j|$.\\ 
\\
Finally, subsitituting gives us: $\overline{C^k_j} = |C_i|$ $mod$ $|C_j| \geq a$.\\
\\
Thus we have shown that, $|C_i|$ $mod$ $|C_j| \geq a$.\\
\textit{Q.E.D}\\ 
\\
\\
\textbf{Lemma}\\
For any $C_i \in [S_{a,n}]$, $|C_i|$ is the least positive integer greater than $|C_{i-1}|$ such that $|C_i|$ $mod$ $|C_j| \geq a$ for all $j < i$.\\
\\
\textit{Proof}\\
Assume you discovered $C_{i-1}$ on step $p$.\\
Assume $k$ is the first step after discovering $C_{i-1}$ such that all cycles are inactive (if we can show that cycle $C_i$ is discovered on step $k$, then our proof will be complete).\\
\\
Then, on all steps, $q$, $p \leq q < k$, there must exist at least one cycle $C_j$ that is active (implying $q + a + n$ $mod$ $|C_j| < a$). Therefore, the ASM algoirthm cannot create cycle $C_i$ on any step $p \leq q < k$.\\
\\
Furthermore, on step $k$, the algorithm must create $C_i$, giving us $\overline{C^k_i} = 0 = k + a + n$ $mod$ $|C_i|$.\\
\\
Thus, since $k + a + n = |C_i|$ is the least positive integer such that $|C_i|$ $mod$ $|C_j| \geq a$ for all $j < i$, and have proved our lemma.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Definition}\\
A positive integer is prime if it's only factors are 1 and itself.\\
\\
\textbf{Axiom}\\
The $i$th prime number, $p_i$, is the least integer that is greater than the $(i-1)th$ prime number and is not divisible by $p_j$ for all $j < i$ (with the first prime number $p_0=2$).\\ 
Note: This is only an axiom because proving the statement is distracting to ASMs.\\
\\
\\
\textbf{Lemma}\\
$|S_{1,1}| = P = \{$ the set of prime numbers $\}$\\
\\
\textit{Proof}\\
$S_{1,1}$ starts with an initial cycle of length $1 + 1 = 2$, thus giving us the first prime number.\\
\\
For all $i, j$ such that $i > j \geq 0$, $C_i$ will have the property that $|C_i|$ $mod$ $|C_j| \geq 1$, equivalenty, $|C_i|$ is not divisible by any of the previous lengths $|C_j|$. Furthermore, $|C_i|$ is the smallest such integer greater than $|C_{i-1}|$ with the property of not being divisible by the previous cycle lengths.\\
\\
Thus, by our axiom, $S_{1,1}$ produces the prime numbers.\\
\textit{Q.E.D}\\ 
\\
\\
\textbf{Theorem}\\
For every ASM, $S$, the set $[S]$ is non-finite.\\
\\
\textit{Proof}\\  
Assume there exists an ASM, $S_{a,n}$ such that $[S_{a,n}]$ is finite.\\
Let $s$ be the number of cycles in $[S_{a,n}]$.\\
\\
Then there exists some step $p$ on which the last cycle, $C_{max}=C_{s-1}$ of the ASM is discovered.\\
\\
We know $C_{max}$ must have at least $n + (s-1)a > (s-1)a$ non-activator nodes.\\
\\
We also know that on step $p$, all cycles $C_{i<s}$ must have been inactive to have discovered $C_{max}$.\\
\\
Furthermore, on step $j$, $C_{max}$'s position is at 0.\\
\\
Let $z = \prod_{i<s}|C_i|$.\\
\\
If we move the $ASM$ $z$ steps, then the position of every $C_{i<z}$ will be maintained since $z$ is a multiple of the length of every $C_{i<z}$.\\
\\
Thus, if after moving $z$ steps from $p$, $C_{max}$ is inactive, then we would need to create a new cycle, and we'd be finished with our proof.\\
\\
Assume $C_{max}$ was active after moving the $z$ steps.\\
\\
Let $n =\overline{C_{max}}$\\
\\
We know $n < a$ since $C_{max}$ is inactive.\\
\\
Note: the position of $C_{max}$ after moving $zt$ steps is the same as after moving $nt$ steps since $z > |C_{max}|$ (if z were less than $|C_{max}|$, then moving $z$ steps would have inactivated $C_{max}$ since $z > a$).\\
\\
Let $j$ be the greatest integer such that $jn < a$.\\
Then your position after moving $jz$ steps will be $a - jn$.\\
Thus, the $(j+1)z$th step will put $C_{max}$ at position $(a-jn)+n \geq a$.\\   
\\
If the $ASM$ is to remain active, then $(a-jn)+n$ must go beyond all $(s-1)a$ non-activator nodes in $C_{max}$.\\
\\
Thus, $(a-jn)+n > a + (s-1)a \geq sa \geq 2a$.\\
\\
But both $n$ and $(a-jn)$ are less then $a$.\\
But that means: $a + a = 2a > (a-jn)+n \geq 2a$, which is a contradiction!\\
\\
Therefore, moving $(j+1)z$ steps from the discovery of $C_{max}$ will maintain the (inactive) positions of all $C_{i<z}$ and also move $C_{max}$ into one of it's inactive nodes, requiring a new cycle to be made, and completing our proof.\\
\textit{Q.E.D}\\   
\\
\\
\textbf{Corollary}\\
There are an infinite number of prime numbers.\\
\\
\textit{Proof}\\ 
$S_{1,1}$ produces the prime numbers, and $[S_{1,1}]$ is non-finite by the above theorem.\\
\textit{Q.E.D}
\end{document}