\documentclass[a4paper,12pt]{article}
\usepackage{amsfonts}
\usepackage{tcolorbox}

\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue
}

\begin{document}

\title{Awkward State Machines}
\author{Will Dengler}
\maketitle

\section{Introduction}
TODO motivate the reader
  
\section{Notations \& Assumed Knowledge}
\label{sec:assumed_knowledge}
The study of awkward state machines requires that we make use of preexisting theory. In this section, we will specify any such knowledge that is required.


\subsection{Notations}
\begin{tcolorbox}
\textbf{Notation: Sets}
\begin{itemize}
\item $\mathbb{Z}$ to represent the set of integers.
\item $\mathbb{N}$ to represent the set of natural numbers, which we will define to be the non-negative integers.
\item $\mathbb{N^{+}}$ to represent the set of positive nature numbers
\item $[n]$ to represent all natural numbers less than $n$.
\item $x \in Y$ to mean $x$ is an element of the set $Y$.
\end{itemize}
\end{tcolorbox}
\label{notation:sets}


\subsection{The Remainder Theorem}
We will need to borrow the concept of the \textit{remainder} from basic number theory. We will not bother proving the associated theorem ourselves, as it is well known, see \url{https://en.wikipedia.org/wiki/Remainder} for a basic overview.\\

\begin{tcolorbox}
\textbf{\hypertarget{theorem:remainder}{The Remainder Theorem}}\\
For any positive integer $n \in \mathbb{N^{+}}$, for any non-negative integer $t \in \mathbb{N}$, there exists a unique integer $q \in \mathbb{N}$, and a unique integer $r \in [n]$ such that $t = qn + r$.\\
\\
We call $r$ the \textit{remainder} of $t$ when divided by $n$.
\end{tcolorbox}
\label{theorem:remainder}
\hypertarget{theorem:remainder}{}
\noindent
\\
It should be noted that there is a wealth more knowledge known about the remainder; however, we will take the time to prove any other results we might need throughout the rest of this paper.


\subsection{Graphs}
\url{https://en.wikipedia.org/wiki/Graph_(discrete_mathematics)}\\
\url{https://en.wikipedia.org/wiki/Directed_graph}

\section{Cycle Graphs}
\label{sec:cycle_graphs}


In order for us to study awkward state machines in any detail, we are going to need to understand their underlying components. As such, let us start by examining the simplest component in an awkward state machine: the cycle graph.



\subsection{Basic Definitions}
\label{sec:cycle_graphs_basic_definitions}

\label{definition:cycle_graph}
\hypertarget{definition:cycle_graph}{}
\begin{tcolorbox}
\textbf{Definition}\\
A \textit{cycle graph} is a directed, connected graph whose points form a circle. More explicitly, for any $n > 1$, the cycle graph $G_n$ has $n$ points:

\begin{center}
$P = \{$ $p_i$ $|$ $i \in [n]$ $\}$
\end{center}
\noindent

such that for any $i < n - 1$, $p_i$ has only one edge which goes to $p_{i + 1}$, and $p_{n-1}$ has only one edge going to $p_0$.
\end{tcolorbox}


\noindent
\\
Our definition tells us that a cycle graph is nothing more than a circular, directed graph with an explicit and straightforward labeling of its points.


\subsubsection{The Walk Functions}

Our study of cycle graphs will be predominantly focused on the outcomes of walks about cycle graphs. As such, for the sake of clarity in our writing, let us now take the time to define two functions - the \textit{walk function} $\omega$ and the \textit{index walk function} $\Omega$ - that we can use to talk about walks of arbitrary length around any cycle graph.\\

\label{definition:walk_functions}
\hypertarget{definition:walk_functions}{}
\begin{tcolorbox}
\textbf{Definition}\\
For any cycle graph $G_n$ with points $P$, we define the \textit{walk function} $\omega: (P, \mathbb{N}) \rightarrow P$ to be $\omega(p_i, k) = p_j$, where $p_j$ is the point arrived at after a walk of length $k$ when starting at point $p_i$ on cycle graph $G_n$.\\
\\
We define $\omega(p_i, 0) = p_i$ for completeness and consistency, even though a walk of length $0$ does not exist. We can instead interpret this as the result of not taking a walk.\\
\\
Furthermore, we define the \textit{walk index function}, $\Omega: (P, \mathbb{N}) \rightarrow [n]$ to be $\Omega(p_i, k) = j$, where $j$ is the index of the point $\omega(p_i, k)$.
\end{tcolorbox}


\noindent
\\
We already know a little bit about the walk and walk index functions from our definition of a cycle graph. Our definition tells us point that $p_{n-1}$ only has one edge going to point $p_0$. Furthermore, for any other point $p_k$ there is only a single edge going to point $p_{k+1}$. Thus, we have:\\


\label{lemma:walk_1}
\hypertarget{lemma:walk_1}{}
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$:
\begin{itemize}
\item $\omega(p_{n-1}, 1) = p_0$ and $\Omega(p_{n-1}, 1) = 0$
\item For any $k < n - 1$, $\omega(p_k, 1) = p_{k+1}$ and $\Omega(p_k, 1) = k + 1$
\end{itemize}
\end{tcolorbox}


\subsection{Validity of the Walk Function}
\label{sec:validatity_of_walk_function}

In order for our mappings $\omega$ and $\Omega$ to be valid mappings, they will have defined for all input; furthermore, each input can only map to a single output. Since our functions map to the outcomes of walks of varying length around cycle graphs, they will only be valid if for any walk length, starting at any point, on any cycle graph, that it must be the case that the walk can be completed; furthermore, there must only be a single walk that can be taken. As such, we shall begin our study by proving just this.\\


\label{lemma:omega_is_valid}
\hypertarget{lemma:omega_is_valid}{}
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any point $p_i$ within that graph, there is only a single walk of length $k$ that begins at point $p_i$.\\
\\
In other words, $\omega$ and $\Omega$ are valid mappings.
\end{tcolorbox}


\noindent
\textit{Proof}\\
The truth to this lemma is result of the fact that every point in a cycle graph only has a single edge. As such, we shall use this fact to complete a proof by induction on the length of our walk.\\

\noindent
\textit{Base Case}\\ 
Our base case demands us to show that there is only a single walk of length one starting at any point, in any cycle graph. But of course this is true because our definition of a cycle graph stipulates that the point $p_{n-1}$ has only a single edge going to point $p_0$, while all other points $p_k$ only have a single edge going to point $p_{k+1}$. As such, a walk of length one is possible from any point; furthermore, there is only one such walk since there is only a single edge that can be traversed.\\

\noindent
\textit{Inductive Hypothesis}\\
Let us assume that for all $1 \leq j < k$, for all cycle graphs, that it is the case that there is only a single walk that can be taken of length $j$ starting from any point within the graph. In other words, that $\omega(p_i, j)$ is defined on all points of all cycle graphs whenever $j < k$.\\

\noindent
\textit{Inductive Step}\\
We must now show that we can complete a walk of length $k$ starting from any point in any cycle graph, and that there is only one walk that we can take to do so.\\

\noindent Our inductive hypothesis tells us that there is only a single walk of length $k - 1$ we can take starting from any point $p_i$, ending at point $\omega(p_i, k - 1)$.\\

\noindent Furthermore, our base case tells us that there is only a single walk of length one that can be taken from any point in our graph. As such, it must be the case that there is only a single walk of length one that can be taken from point $\omega(p_i, k - 1)$; equivalently we know $\omega(\omega(p_i, k - 1), 1)$ is defined.\\

\noindent Since our composite walk $\omega(\omega(p_i, k - 1), 1)$ is composed of a walk of length $k - 1$ and a second walk of length $1$, our composite walk has length $k$. Furthermore, since both components of our walk were the only walks that could have been taken, we can conclude that our composite walk is the only walk that could have been taken of length $k$. As such, not only have we completed our proof, but we have also shown that:

\begin{center}
$\omega(p_i, k) = \omega(\omega(p_i,$ $k - 1),$ $1)$ whenever $k > 1$
\end{center}

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center} 




\subsection{Composition of Walks}
\label{sec:composition_of_walks}

In order to show a walk of a certain length has some property, we will often need to ``split up" the walk into two or more shorter walks and then reason about the composition of the shorter walks in order to reason about the original longer walk.\\

\noindent To start, we will show that a walk of any length $j$ is equivalent to first taking a walk of length $t < j$, followed by a walk of length $j - t$; or first taking a walk of length $j - t$, followed by a walk of length $t$.\\


\label{lemma:composition_of_sub_walks}
\hypertarget{lemma:composition_of_sub_walks}{}
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any point $p_i$, for any integers $0 \leq t \leq j,$:
\begin{center}
$\omega(p_i, j)$ $=$ $\omega(\omega(p_i, t), j - t)$ $=$ $\omega(\omega(p_i, j - t), t)$. 
\end{center}
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent We know there exists a single walk of length $t$ starting at point $p_i$.
Let $p_w$ be the point this walk ends at, then $p_w = \omega(p_i, t)$.\\

\noindent We know there exists a single walk of length $j - t$ starting from point $p_w$.
Let $p_y$ be the point this walk ends at, then $p_y = \omega(p_w, j - t)$.\\
Applying substitution yields:

\begin{center}
$p_y = \omega(\omega(p_i, t), j - t)$
\end{center}

\noindent Combining the two above walks yields a walk from $p_i$ to $p_y$ of length\\
$t + (j - t) = j$. Thus, we have shown:
\begin{center}
$p_y = \omega(p_i, j) = \omega(\omega(p_i, t), j - t)$
\end{center}

\noindent To complete our proof, we need to show:
\begin{center}
$\omega(p_i, j) = \omega(\omega(p_i, j - t), t)$.
\end{center}

\noindent Let us define $a = j - t$.\\

\noindent From the first part of this proof, we know that
\begin{center}
$\omega(p_i, j) = \omega(\omega(p_i, a), j - a)$
\end{center}

\noindent Substituting out $a$ yields:
\begin{center}
$\omega(p_i, j) = \omega(\omega(p_i, j - t), j - (j - t)) = \omega(\omega(p_i, j - t), t)$.
\end{center}

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}


\label{corollary:composition_of_walks}
\hypertarget{corollary:composition_of_walks}{}
\begin{tcolorbox}
\textbf{Corollary}\\
For any cycle graph $G_n$, for any point $p_i$, for any non-negative integers $t, v$:
\begin{center}
$\omega(p_i, t + v) = \omega(\omega(p_i, t), v) = \omega(p_i, v), t)$
\end{center}
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent Let $j = t + v$. Through the \hyperlink{lemma:composition_of_sub_walks}
{previous lemma}, we know that
\begin{center}
$\omega(p_i, t + v) = \omega(p_i, j) = \omega(\omega(p_i, t), j - t)$\\
$\omega(p_i, t + v) = \omega(p_i, j) = \omega(\omega(p_i, j - t), t)$
\end{center}

\noindent Substitution brings us:
\begin{center}
$\omega(p_i, t + v) = \omega(\omega(p_i, t), (t + v) - t) = \omega(\omega(p_i, t), v)$
$\omega(p_i, t + v) = \omega(\omega(p_i, (t + v) - t), t) = \omega(\omega(p_i, v), t)$
\end{center}


\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}



\subsection{Shortest Walks}
\label{sec:shortest_walks}

Now that we have the ability to compose walks around our cycle graph, we will now show that from any point $p_i$ that there exists a walk to any other point $p_j$. Furthermore, we will be able to determine what the length of shortest possible walk from point $p_i$ to $p_j$.\\


\label{lemma:existence_of_walk_1}
\hypertarget{lemma:existence_of_walk_1}{}
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any integers $i, j$ such that
\begin{center}
$0 \leq j \leq i < n$
\end{center}

\noindent then
\begin{center}
$\omega(p_j, i - j) = p_i$
\end{center}
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent This will be a proof by induction on the length of the walk $l = i - j$.\\

\noindent
\textit{Base Case} ($length = 0$)

\noindent Let $l = 0$ be the length of the walk, giving us $l = i - j = 0$ and $i = j$.

\noindent Substituting gives us:
\begin{center}
$\omega(p_j, l) = \omega(p_j, i - j) = \omega(p_j, 0) = p_j = p_i$
\end{center} 

\noindent
\textit{Inductive Hypothesis}\\
Assume for all $l < t \leq n - l$, that $\omega(p_j, l) = \omega(p_j, i - j) = p_i$\\

\noindent
\textit{Inductive Step}\\
We need to show that $\omega(p_j, t) = \omega(p_j, i - j) = p_i$.\\

\noindent By \hyperlink{lemma:composition_of_sub_walks}{previous lemma}, we know that:
\begin{center}
$\omega(p_j, t) = \omega(\omega(p_j, t - 1), 1)$
\end{center}

\noindent Focusing on the inner function and substituting:
\begin{center}
$\omega(p_j, t - 1) = \omega(p_j, (i - j) - 1) = \omega(p_j, (i - 1) - j)$
\end{center}

\noindent Applying the inductive hypothesis:
\begin{center}
$\omega(p_j, t - 1) = \omega(p_j, (i - 1) - j) = p_{i-1}$
\end{center}

\noindent Substituting $p_{i-1}$ back into $\omega(p_j, t)$ yields:
\begin{center}
$\omega(p_j, t) = \omega(\omega(p_j, t - 1), 1) = \omega(p_{i-1}, 1) = p_i$
\end{center}

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}


\noindent Now that we know there exists a walk of length $j - i$ from point $p_i$ to point $p_j$ whenever $i \leq j$, we will now show that this is the shortest possible such walk whenever $i \neq j$. We exclude the case where $i = j$ since there is no such thing as a walk of length $0$, even though $\omega(p_i, 0 = i - j) = p_i$.\\



\label{corollary:shortest_walk_1}
\hypertarget{corollary:shortest_walk_1}{}
\begin{tcolorbox}
\textbf{Corollary}\\
For any cycle graph $G_n$, for any integers $i, j$ such that

\begin{center}
$0 \leq j < i < n$
\end{center}

\noindent the shortest walk from $p_j$ to $p_i$ has length $i - j$.
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent We will complete our proof by assuming we have a shorter walk, and then proving that assumption leads to a contradiction.\\

\noindent Before diving in, we can immediately eliminate the case where $i = j + 1$ since there is no such thing as a walk with a length less than $1$.\\

\noindent Let $l = i - j \geq 2$. Assume there exists a shorter walk of length $k$ from point $p_j$ to point $p_i$.\\

\noindent Since $k < l$, there must exist some integer $1 \leq t < l$ such that $k = l - t$.

\noindent Substituting yields:
\begin{center}
$k = l - t = (i - j) - t = (i - t) - j$
\end{center}

\noindent Since $t < l = i - j$, then $t - i < j$. Multiplying by -$1$ gives $i - t > j$.\\

\noindent As such we can apply the \hyperlink{lemma:existence_of_walk_1}{previous lemma}:
\begin{center}
$\omega(p_j, k) = \omega(p_j, (i - t) - j) = p_{i - t}$.
\end{center}

\noindent However, by assumption $\omega(p_j, k) = p_i \neq p_{i - t}$ since $1 \leq t$. Thus we have reached our contradiction.

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}




\label{corollary:Omega_result_1}
\hypertarget{corollary:Omega_result_1}{}
\begin{tcolorbox}
\textbf{Corollary}\\
For any cycle graph $G_n$, for any non-negative integers $i, j$ such that $i + j < n$,
\begin{center}
$\Omega(p_i, j) = i + j$
\end{center}
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent Since we have that $i \leq i + j < n$, the \hyperlink{lemma:existence_of_walk_1}{previous lemma} tells us:
\begin{center}
$\omega(p_i, (i + j) - i) = p_{i+j}$
\end{center}

\noindent We can simplify $(i + j) - i = j$, thus we have:
\begin{center}
$\omega(p_i, j) = p_{i+j}$ and 
$\Omega(p_i, j) = i + j$.
\end{center}


\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}


\noindent We now have a strong understanding about walks from any point $p_i$ to any other point $p_j$ where $i \leq j$. As such, we shall now move on to examine walks where $j \leq i$.\\



\label{lemma:existence_of_walk_2}
\hypertarget{lemma:existence_of_walk_2}{}
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any integers $i, j$ such that
\begin{center}
$0 \leq i \leq j < n$
\end{center}

\noindent then
\begin{center}
$\omega(p_j, (n - j) + i) = p_i$
\end{center}
\end{tcolorbox}

\noindent
\textit{Proof}\\
We shall prove this directly. Using the \hyperlink{lemma:existence_of_walk_1}{previous lemma}, we know that:
\begin{center}
$\omega(p_j, (n - 1) - j) = p_{n-1}$ since $j \leq (n - 1)$
\end{center}

Furthermore, we know that:
\begin{itemize}
\item $\omega(p_{n-1}, 1) = p_0$ by structure of $G_n$
\item $\omega(p_0, i) = \omega(p_0, i - 0) = p_i$ by the \hyperlink{lemma:existence_of_walk_1}{previous lemma} since $0 \leq i$
\end{itemize}

\noindent As such, we can create a composite of these three walks to form a walk from $p_j$ to $p_i$ having length:

\begin{center}
$((n - 1) - j) + 1 + i = (n - j) + i$
\end{center}

\noindent which completes our proof.

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}




\label{corollary:Omega_result_2}
\hypertarget{corollary:Omega_result_2}{}
\begin{tcolorbox}
\textbf{Corollary}\\
For any cycle graph $G_n$, for any point $p_i$, for any non-negative integer $j \leq n$ such that $i + j \geq n$, then

\begin{center}
$\Omega(p_i, j) = j - (n - i) = (i + j) - n$
\end{center}
\end{tcolorbox}

\noindent
\textit{Proof}\\
If we can show that $0 \leq j - (n - i) \leq i$, then the \hyperlink{lemma:existence_of_walk_2}{previous lemma} tells us that:
\begin{center}
$\Omega(p_i, (n - i) + (j - (n - i))) = j - (n - i)$
\end{center}

\noindent Simplifying our expression yields:
\begin{center}
$(n - i) + (j - (n - i)) = (n - i) - (n - i) + j = j$
\end{center}

\noindent Thus, if we can show $0 \leq j - (n - i) \leq i$, then we will have:
\begin{center}
$\Omega(p_i, (n - i) + (j - (n - i))) = \Omega(p_i, j) = j - (n - i)$
\end{center}

\noindent We can subtract $i$ from our inequality, $i + j \geq n$, to get $j \geq n - i$. As such, it is the case that
\begin{center}
$j - (n - i) \geq (n - i) - (n - i) \geq 0$
\end{center}

\noindent Now we just need to show that $j - (n - i) \leq i$ to complete our proof. The truth of this follows from the fact that $j \leq n$:
\begin{center}
$j - (n - i) = (j + i) - n \leq (n + i) - n = i$
\end{center}

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}



\label{corollary:shortest_walk_2}
\hypertarget{corollary:shortest_walk_2}{}
\begin{tcolorbox}
\textbf{Corollary}\\
For any cycle graph $G_n$, for any integers $i, j$ such that
\begin{center}
$0 \leq i \leq j \leq n - 1$
\end{center}
the shortest walk from $p_j$ to $p_i$ has length $(n - j) + i$.
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent We will complete this proof by showing that $\omega(p_j, l) \neq p_i$ for all $l < (n - j) + i$.\\

\noindent We will start by assuming we have $l \leq (n - j)$. For any such $l$, we can find some $k$ such that $l = (n - j) - k$ where $0 \leq k < n - j$. Substituting yields:
\begin{center}
$\omega(p_j, l) = \omega(p_j, (n - j) - k) = \omega(p_j, (n - k) - j)$
\end{center}

\noindent By a \hyperlink{lemma:existence_of_walk_1}{previous lemma}, we know that $\omega(p_j, (n - k) - j) = p_{n - k}$ as long as $j \leq n - k < n$.\\

\noindent We know that $k < n - j$ by our selection of $l$. Adding $j$ yields $k + j < n$, followed by subtracting $k$ give us $j < n - k$. Thus, we are able to apply our lemma to give us $\omega(p_j, (n - k) - j) = p_{n - k}$. Furthermore, we know $i \leq j < n - k$, thus we have that $i \neq n - k$, yielding:
\begin{center}
$\omega(p_j, l) = \omega(p_j, (n - k) - j) = p_{n-k} \neq p_i$.
\end{center}

\noindent As such, we have eliminated all $l \leq (n - j)$.\\

\noindent We now need to show that if $l$ is within: $n - i < l < (n-1) + i$, then $\omega(p_j, l) \neq p_i$.\\

\noindent For any $l$, we can find some $k$ such that $l = (n - j) + k$ where $1 \leq k < i$. Substituting allows us to apply the previous lemma:
\begin{center}
$\omega(p_j, l) = \omega(p_j, (n - j) + k) = p_k \neq p_i$.
\end{center}

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}



\label{corollary:inverse_omega_relationship}
\hypertarget{corollary:inverse_omega_relationship}{}
\begin{tcolorbox}
\textbf{Corollary}\\
For any cycle graph $G_n$, for any points $p_i, p_j$, there exists positive integers $a, b \leq n$ such that:
\begin{center}
$\omega(p_i, a) = p_j$ and
$\omega(p_j, b) = p_i$.
\end{center}

\noindent In other words, it is always possible to walk from any point $p_i$ to any point $p_j$ with a length no greater than $n$.\\

\noindent Furthermore, whenever $p_j \neq p_i$, then the integers $a, b$ will satisfy the relationship: $a = n - b$ (equivalently $b = n - a$).\\

\noindent Finally, whenever $p_j = p_i$, then we will have $a = b = n$.
\end{tcolorbox}


\noindent
\textit{Proof}

\noindent The first part of this proof follows directly from the previous two lemmas.\\

\noindent Let $p_i, p_j$ be any two points such that $i < j$. We know that $\omega(p_i, j - i) = p_j$ by the \hyperlink{lemma:existence_of_walk_1}{first of our two previous lemmas}. Furthermore, $j - i < j < n$.\\

\noindent Now assume that $j \leq i$. We know that $\omega(p_i, (n - i) + j) = p_j$ by the \hyperlink{lemma:existence_of_walk_2}{second of our two previous lemmas}. Furthermore, $(n - i) + j \leq (n - i) + i = n$.\\

\noindent As such, we completed showing that from any point $p_i$, there exists a walk to any other point $p_j$ with a length less than or equal to $n$.\\

\noindent We will now prove that whenever $p_j \neq p_i$, that $b = n - a$.\\

\noindent Assume that $i < j$. We know that $\omega(p_i, j - i) = p_j$, and $\omega(p_j, (n - j) + i) = p_i$. Therefor, it must be the case that
\begin{center}
$a = j - i$\\
$b = (n - j) + i = n - (j - i) = n - a$
\end{center}

\noindent We will conclude this proof by showing that a walk of length $n$ from any point $p_i$ will return to point $p_i$.\\

\noindent By the \hyperlink{lemma:existence_of_walk_2}{previous lemma}, we know a walk of length $(n - i) + i) = n$ will end on point $p_i$ since $i \leq i$.

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}




\subsection{Repetition of Walks}
\label{sec:repetition_of_walks}

We have been focused on the result of walks with length no greater than $n$, the number of points within our graph. We will now move onto considering what happens when we take a walk with length greater than $n$.\\


\label{lemma:walk_repetition_of_n}
\hypertarget{lemma:walk_repetition_of_n}{}
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any point $p_i$, for any integers $t \geq 0, a \geq 1$
\begin{center}
$\omega(p_i, t) = \omega(p_i, t + an)$
\end{center}
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent We shall complete this proof by induction on $a$.\\


\noindent
\textit{Base Case} $(a = 1)$\\
Let $p_i$ be any point in our graph, let $t \geq 0$, and let $p_k = \omega(p_i, t)$.\\

\noindent By the \hyperlink{corollary:inverse_omega_relationship}{previous corollary}, we know that $\omega(p_k, n) = p_k$. As such, the composition of these walks is $\omega(\omega(p_i, t), n) = \omega(p_i, t + n) = p_k = \omega(p_i, t)$, thus we have completed our base case.\\


\noindent
\textit{Inductive Hypothesis}\\
Assume for all $1 \leq a < l$ for some $l$, that $\omega(p_i, t) = \omega(p_i, t + an)$.\\


\noindent
\textit{Inductive Step}\\
We need to show that $\omega(p_i, t + ln) = \omega(p_i, t)$.\\

\noindent By previous lemma, we know:
\begin{center}
$\omega(p_i, t + ln) = \omega(\omega(p_i, n), t + (l - 1)n)$
\end{center}

\noindent By the \hyperlink{corollary:inverse_omega_relationship}{previous corollary}, we know $\omega(p_i, n) = p_i$. As such, we can substitute:
\begin{center}
$\omega(\omega(p_i, n), t + (l - 1)n) = \omega(p_i, t + (l - 1)n)$
\end{center}

\noindent Since $l - 1 < l$, we can apply our inductive hypothesis to complete our proof
\begin{center}
$\omega(p_i, t + (l - 1)n = \omega(p_i, t)$
\end{center}


\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}



\subsection{Remainders}
\label{sec:remainders}

We will often wish to refer to \hyperlink{theorem:remainder}{remainder} throughout our study of awkward state machines. However, it would be rather clumsy to have to constantly to write out that ``$r$ is the remainder of $t$ when divided $n$" to make reference to our definition. As such, let us define a new function, $\rho$, that will allow us to reference the remainder without all the fuss.\\


\begin{tcolorbox}
\textbf{Definition}\\
For any positive integer $n \in \mathbb{N^{+}}$, for any integer $t \in \mathbb{N}$, the \textit{remainder function}, $\rho: (\mathbb{N^{+}}, \mathbb{N}) \rightarrow [n]$, maps $(n, t)$ to the remainder of $t$ when divided by $n$.\\

\noindent As such, if $r$ is the remainder of $t$ when divided by $n$, then we write:
\begin{center}
$\rho(n, t) = r$
\end{center}
\end{tcolorbox}


\noindent Let us quickly note that combining the \hyperlink{theorem:remainder}{remainder theorem} with the remainder function yields: for any positive integer $n$, and any integer $t \geq 0$, there exists a unique integer $q \geq 0$ such that $t = qn + \rho(n, t)$.\\


\label{lemma:remainder_subtraction}
\hypertarget{lemma:remainder_subtraction}{}
\begin{tcolorbox}
\textbf{Lemma}\\
For any $i \in [n]$, if $i = j - qn$ for some integers $j, q \in \mathbb{N}$, then $\rho(n, j) = i$.
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent Assume $i \in [n]$ and $i = j - qn$ for some integers $j, q \in \mathbb{N}$.\\

\noindent Solving for $j$ yields. $j = i + qn$.\\

\noindent Since $i < n$, then $i$ and $q$ must be the unique integers from the remainder theorem. As such $i$ is defined to be remainder of $j$ when divided by $n$. 
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}



\subsection{Omega Walk Theorem}
\label{sec:omega_walk_theorem}

\noindent TODO: Add background and motivation\\

\label{lemma:omega_rho_reduction}
\hypertarget{lemma:omega_rho_reduction}{}
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any point $p_i$, for any integer $t \geq 0$
\begin{center}
$\omega(p_i, t) = \omega(p_i, \rho(n, t))$
\end{center}
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent We know there exists some integer $q \geq 0$ such that $t = qn + \rho(n,t)$. As such, we can use this expression to substitute for $t$:
\begin{center}
$\omega(p_i, t) = \omega(p_i, qn + \rho(n, t)) = \omega(\omega(p_i, qn), \rho(n, t))$
\end{center}

\noindent By previous lemma, we know that:
\begin{center}
$\omega(p_i, qn) = \omega(p_i, qn + 0) = \omega(p_i, 0) = p_i$
\end{center}

\noindent As such, we can substitute in $p_i$ to complete our proof:
\begin{center}
$\omega(p_i, t) = \omega(\omega(p_i, qn), \rho(n, t)) = \omega(p_i, \rho(n, t))$
\end{center}

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}


\noindent We have now accumulated enough knowledge about cycle graphs with a theorem.\\



\label{theorem:omega_walk_theorem}
\hypertarget{theorem:omega_walk_theorem}{}
\begin{tcolorbox}
\textbf{Omega Walk Theorem}

\noindent For any cycle graph $G_n$, for any point $p_i$, for any $t \in \mathbb{N}$
\begin{center}
$\Omega(p_i, t) = \rho(n, i + t)$
\end{center}

\noindent In other words, if a walk of length $t$ starting at point $p_i$ ends on point $p_j$, then $j = \rho(n, i + t)$.
\end{tcolorbox}


\noindent
\textit{Proof}

\noindent Let $p_i$ be any point. Let $t \in \mathbb{N}$.\\

\noindent Let $q \in N$ be the integer such that $t = \rho(n, t) + qn$. Then $\rho(n, t) = t - qn$.\\

\noindent The \hyperlink{lemma:omega_rho_reduction}{previous lemma} tells us that $\Omega(p_i, t) = \Omega(p_i, \rho(n, t))$.\\

\noindent Since $\rho(n, t) < n$, our previous corollaries tell us that
\begin{itemize}
\item $\Omega(p_i, \rho(n, t)) = i + \rho(n, t)$ whenever $i + \rho(n, t) < n$ by \hyperlink{corollary:Omega_result_1}{corollary}
\item $\Omega(p_i, \rho(n, t)) = \rho(n, t) - (n - i)$ whenever $i + \rho(n, t) \geq n$ by \hyperlink{corollary:Omega_result_2}{corollary}
\end{itemize}

\noindent As such, we need to show that both of these cases are in fact equal to $\rho(i + t)$.\\


\noindent
\textit{Case 1}

\noindent Let $\rho(n, t) + i < n$, then 
\begin{center}
$\Omega(p_i, \rho(n, t)) = i + \rho(n, t) = i + (t - qn) = (i + t) - qn$
\end{center}
Since $\Omega(p_i, \rho(n, t)) \in [n]$, then $\Omega(p_i, \rho(n, t)) = \rho(i + t)$ by \hyperlink{lemma:remainder_subtraction}{previous lemma}.\\


\noindent
\textit{Case 2}

\noindent Let $\rho(n, t) + i \geq n$, then 
\begin{center}
$\Omega(p_i, \rho(n, t)) = \rho(n, t) - (n - i) = (t - qn) - (n - i) = (i + t) - (q + 1)n$
\end{center}

\noindent Since $\Omega(p_i, \rho(n, t)) \in [n]$, then $\Omega(p_i, \rho(n, t)) = \rho(i + t)$ by \hyperlink{lemma:remainder_subtraction}{previous lemma}.


\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}







\section{Cycle Machines}
\label{section:cycle_machines}
\hypertarget{section:cycle_machines}{}

\noindent In this section, we will begin working with the \textit{cycle machine}, a state-based machine which can be represented using a cycle graph.\\




\label{definition:cycle_machine}
\hypertarget{definition:cycle_machine}{}
\begin{tcolorbox}
\textbf{Definition}\\
For any integer $n \geq 2$, we define the \textit{cycle machine}, $C_n$, to be a state machine with $n$ states $S = \{$ $s_i$ $|$ $i \in [n]$ $\}$ such that:

\begin{itemize}
\item The initial state of the machine is always $s_0$.

\item For any $i < n - 1$, if the current state of the cycle machine is $s_i$, then the subsequent state is defined to be $s_{i+1}$

\item If the current state of the cycle machine is $s_{n - 1}$, then the subsequent state is defined to be $s_0$.
\end{itemize}

\noindent When we move from one state to the next, we say that we've completed one \textit{transition} of the state machine.\\

\noindent If a cycle machine is on some state $s_i$ after completing $k \in \mathbb{N}$ subsequent transitions from the initial state, then we say that the cycle machine has position $i$ on step $k$.\\
\end{tcolorbox}



\noindent \\
We will be interested in determining the state of any cycle machine on any step. As such, let us define the \textit{step function} to allow us to more easily refer to this value within our study.\\









\label{definition:cycle_step_function}
\hypertarget{definition:cycle_step_function}{}
\begin{tcolorbox}
\textbf{Definition}

\noindent For any cycle machine $C_n$, for any $k \in \mathbb{N}$, the \textit{step function}, $\phi(C_n, k)$, is defined to be state of $C_n$ on step $k$.
\end{tcolorbox}



\noindent \\
If we wish to use the step function, we will need to show that it is well defined. In other words, we need to show that any cycle graph can be undergo an arbitrary number of transitions from the initial state, and when doing so, the machine will always end on some unique state $s_i$.\\

\noindent To start, let us first show that $\phi(C_n, 0)$ is well defined for any cycle machine.\\







\label{lemma:phi_of_0}
\hypertarget{lemma:phi_of_0}{}
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle machine $C_n$, $\phi(C_n, 0) = s_0$.
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent By \hyperlink{definition:cycle_machine}{definition}, the initial state of any cycle machine $C_n$ is state $s_0$.\\

\noindent Furthermore, $\phi(C_n, 0)$ is defined to be the state of $C_n$ after completing $0$ subsequent transitions from the initial state.\\

\noindent If we were to take $0$ transitions from our initial state, then we would not have transitioned states at all. As such, we would still be at our initial state, $s_0$.

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}









\label{lemma:phi_well_defined}
\hypertarget{lemma:phi_well_defined}{}
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle machine $C_n$, for any $k \in \mathbb{N}$, $\phi(C_n, k)$ is well defined.
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent We shall complete this proof by induction on the number of transitions, $k$.\\


\noindent
\textbf{Base Case} $k = 0$

\noindent By \hyperlink{lemma:phi_of_0}{the previous lemma}, $\phi(C_n, 0) = s_0$ for any cycle machine $C_n$.\\


\noindent
\textbf{Induction Hypothesis}

\noindent Assume for all $0 \leq j < k$, that $\phi(C_n, j)$ is well defined.\\


\noindent
\textbf{Inductive Step}

\noindent By definition of $\phi$, $\phi(C_n, k - 1)$ is defined to be the state of $C_n$ after $k - 1$ subsequent transitions from the initial state $s_0$.\\

\noindent By the inductive hypothesis, $\phi(C_n, k - 1)$ is well defined.\\

\noindent As such, $\phi(C_n, k - 1)$ exists and maps to a single state.\\

\noindent Let $s_i$ be the state such that $s_i = \phi(C_n, k - 1)$.\\

\noindent If we can show that for any $s_i$, that there exists a single subsequent state $s_j$, then $s_j$ will have to be the state of $C_n$ after completing $(k - 1) + 1 = k$ subsequent transitions from the initial state. As such, $s_j = \phi(C_n, k)$, and we will have completed our proof.\\ 


\noindent
\textbf{Case:} $i = n - 1$

\noindent By definition of a cycle machine, $s_{n-1}$ has a single subsequent state defined to be $s_0$.\\


\noindent
\textbf{Case:} $i < n - 1$

\noindent By definition of a cycle machine, $s_{i+1}$ is the only subsequent state of state $s_i$ when $i < n - 1$.

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}




\noindent \\
As we've seen in our proofs this section, we will often be interested in referring to the state of some cycle machine $C_n$ after completing one or more transitions from some step. As such, let us define the \textit{state function} so that we can more easily reference the resulting state.\\






\label{definition:state_function}
\hypertarget{definition:state_function}{}
\begin{tcolorbox}
\textbf{Definition}

\noindent For any cycle machine $C_n$, for any state $s_i$, for any $j, k \in \mathbb{N}$, we define the \textit{state function}, $\psi(C_n, j, k)$, to be the state of cycle machine $C_n$ after completing $k$ transitions from step $j$.\\

\noindent Furthermore, we define the \textit{state position function}, $\Psi(C_n, j, k)$ to be index of the state $\psi(C_n, j, k)$.
\end{tcolorbox}




\noindent \\ To show that $\psi$ is also well defined, we will show that it is merely an alias for the step function.\\





\label{lemma:psi_well_defined}
\hypertarget{lemma:psi_well_defined}{}
\begin{tcolorbox}
\textbf{Lemma}

\noindent For any cycle machine $C_n$, for any state $s_i$, for any $j, k \in \mathbb{N}$:

\begin{center}
$\psi(C_n, j, k) = \phi(C_n, j + k)$
\end{center}

\end{tcolorbox}


\noindent
\textit{Proof}

\noindent By definition, $\phi(C_n, j)$ is the state of $C_n$ on step $j$.\\

\noindent If we were to complete $k$ more transitions from $\phi(C_n, j)$, we will have completed a total of $k + j$ from the initial state of $C_n$. Thus, we'd be on state $\phi(C_n, j + k)$ by definition of the step function.\\

\noindent Furthermore, since we arrived at $\phi(C_n, j + k)$ by taking $k$ transitions from step $j$, then $\psi(C_n, j, k) = \phi(C_n, j + k)$ by definition of the state function.

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}





\noindent Now that we have the step and state functions to simplify working with cycle machines, let us now tie cycle machines to our cycle graphs from the previous section. To start, we will define the \textit{bridge function} which will map the points of the cycle graph $G_n$ to the states of the cycle machines $C_n$.\\




\label{definition:bridge_function}
\hypertarget{definition:bridge_function}{}
\begin{tcolorbox}
\textbf{Definition}

\noindent For any cycle machine $C_n$, with states $S$, we define the \textit{bridge function}, $\beta$, from the points of the cycle graph $G_n$ to the states of cycle machine $C_n$ as $\beta(p_i) = s_i$.

\end{tcolorbox}







\label{lemma:bridge_is_well_defined}
\hypertarget{lemma:bridge_is_well_defined}{}
\begin{tcolorbox}
\textbf{Lemma}

\noindent For any point in cycle graph $G_n$, the bridge function is well defined.

\end{tcolorbox}


\noindent
\textsl{Proof}

\noindent Let $p_i$ be any point within the cycle graph $G_n$.\\

\noindent The bridge function is well defined for point $p_i$ if state $s_i$ exists within the cycle machine $C_n$.\\

\noindent By definition of a cycle graph, $i \in [n]$.\\

\noindent By definition, the cycle machine $C_n$ has states $S =$ $\{$ $s_j$ $|$ $j \in [n]$ $\}$.\\

\noindent Thus, $s_i \in S$ since $i \in [n]$.

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}






\noindent Now that we have a bridge from cycle graphs to cycle machines, we will show that transitioning states within the cycle machine $C_n$ can instead be accomplished by taking a walk of the cycle graph $G_n$ and then using the bridge function to return to the states of $C_n$.\\






\label{theorem:bridge_theorem}
\hypertarget{theorem:bridge_theorem}{}
\begin{tcolorbox}
\textbf{Bridge Theorem}\\
For any cycle machine $C_n$, for any $k \in \mathbb{N}$, $\phi(C_n, k) = \beta(\omega(p_0, k))$
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent We will complete this proof by induction on the step, $k$.\\


\noindent
\textbf{Base Case} $k = 0$

\noindent We need to show $\phi(C_n, 0) = \beta(\omega(p_0, 0))$.\\

\noindent By \hyperlink{definition:walk_functions}{definition of $\omega$}, $\omega(p_0, 0) = p_0$.\\

\noindent By \hyperlink{definition:bridge_function}{definition of $\beta$}, $\beta(p_0) = s_0 = \beta(\omega(p_0, 0))$.\\

\noindent By \hyperlink{lemma:phi_of_0}{previous lemma}, $\phi(C_n, 0) = s_0 = \beta(\omega(p_0, 0))$.\\


\noindent
\textbf{Inductive Hypothesis}

\noindent Assume for all $0 \leq j < k$, that $\phi(C_n, j) = \beta(\omega(p_0, j))$.\\


\noindent
\textbf{Inductive Step}

\noindent By definition of $\phi$, $\phi(C_n, k)$ is defined to be the state of $C_n$ after $k$ transitions from the initial state.\\

\noindent Furthermore, $\phi(C_n, k - 1)$ is defined to be the state of $C_n$ after $k - 1$ transitions from the initial state.\\

\noindent Thus, if we first complete $k - 1$ transitions from the initial state to be on state $\phi(C_n, k - 1)$, and then advance one more transition, we will have completed $k$ transitions from the initial state. Thus, the resulting state would be equal to $\phi(C_n, k)$.\\

\noindent $\phi(C_n, k - 1) = \beta(\omega(p_0, k - 1))$ by inductive hypothesis.\\

\noindent Let $i \in [n]$, be the integer such that $\omega(p_0, k - 1) = p_i$.\\

\noindent Then $\phi(C_n, k - 1) = \beta(p_i) = s_i$ by definition of $\beta$.\\


\noindent
\textbf{Case:} $i < n - 1$

\noindent The subsequent state of $s_i$ is $s_{i+1}$ by definition of a cycle machine.\\

\noindent Thus, $\phi(C_n, k) = s_{i+1}$.\\

\noindent By previous lemma, $\omega(p_0, k) = \omega(\omega(p_0, k - 1), 1) = \omega(p_i, 1) = p_{i+1}$.\\

\noindent As such, $\beta(\omega(p_0, k)) = \beta(p_{i+1}) = s_{i + 1} = \phi(C_n, k)$.\\


\noindent
\textbf{Case:} $i = n - 1$

\noindent The subsequent state of $s_{n-1}$ is $s_0$ by definition of a cycle machine.\\

\noindent Thus, $\phi(C_n, k) = s_0$.\\

\noindent By previous lemma, $\omega(p_0, k) = \omega(\omega(p_0, k - 1), 1) = \omega(p_{n-1}, 1) = p_0$.\\

\noindent As such, $\beta(\omega(p_0, k)) = \beta(p_0) = s_0 = \phi(C_n, k)$.\\


\noindent 
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}










\label{corollary:bridged_state_function}
\hypertarget{corollary:bridged_state_function}{}
\begin{tcolorbox}
\textbf{Corollary}\\
For any cycle machine $C_n$, for any $j, k \in \mathbb{N}$, $\psi(C_n, j, k) = \beta(\omega(p_0, j + k))$
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent By \hyperlink{lemma:psi_well_defined}{previous lemma}, $\psi(C_n, j, k)$ = $\phi(C_n, j + k)$.\\

\noindent By the \hyperlink{theorem:bridge_theorem}{previous theorem}, $\phi(C_n, j + k) = \beta(\omega(p_0, j + k))$.

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}






\label{theorem:cycle_machine_remainder_theorem}
\hypertarget{theorem:cycle_machine_remainder_theorem}{}
\begin{tcolorbox}
\textbf{Cycle Machine Remainder Theorem}\\
For any cycle machine $C_n$, for any $j, k \in \mathbb{N}$, $\Psi(C_n, j, k) = \rho(n, j + k)$
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent
 
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}





\section{Cycle Graph State Function}



\label{definition:state_position_function}
\hypertarget{definition:state_position_function}{}
\begin{tcolorbox}
\textbf{Definition}\\
For any cycle graph $G_n$ with points $P = \{ p_i | i \in [n] \}$, we define the state position function $\psi : (G_n, \mathbb{N}, \mathbb{N}) \rightarrow P$ as:

\begin{center}
$\psi(G_n, j, k) = \omega(p_0, j + k)$ for any $j, k \in \mathbb{N}$.
\end{center}

\noindent The first integer argument to $\psi$ is called the \textit{step}; and the second integer argument is referred to as the \textit{count}.\\

\noindent If $\psi(G_n, j, k) = p_i$, we say $p_i$ is the \textit{position} of $G_n$ after $k$ transitions from step $j$.\\

\noindent For simplicity, we will often specify we are working with some specific cycle graph $G_n$ and exclude it as an argument to $\psi$.
\end{tcolorbox}




\noindent
\\
The state position function is fairly: it simply reduces down to an $\omega$ function with the fixed point $p_0$. As such, since we know $\omega$ is well defined, we immediately know that $\psi$ is also well defined.\\

\noindent Before we begin studying the state position function in detail, let us quickly define one more function, $\Psi$.\\




\label{definition:state_function}
\hypertarget{definition:state_function}{}
\begin{tcolorbox}
\textbf{Definition}\\
For any cycle graph $G_n$, for any $j, k \in \mathbb{N}$, we define the \textit{state function} $\Psi : (G_n, \mathbb{N}, \mathbb{N}) \rightarrow [n]$
to be the index of the point $\psi(G_n, j, k)$.\\

\noindent If $\Psi(G_n, j, k) = t$, we say $G_n$ is on state $t$ after $k$ transitions from step $j$.\\

\noindent For simplicity, we will often specify we are working with some specific cycle graph $G_n$ and exclude it as an argument to $\Psi$.
\end{tcolorbox}
\noindent



\label{definition:state_function}
\hypertarget{definition:state_function}{}
\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle graph $G_n$, for any integers $j,k \in \mathbb{N}$:

\begin{center}
$\Psi(G_n, j, k) = \rho(n, j + k)$
\end{center}

\end{tcolorbox}

\noindent
\textit{Proof}

\noindent This proof follows directly from the our definitions of $\Psi$, $\psi$, and $\Omega$ as well as the \hyperlink{theorem:omega_walk_theorem}{Omega Walk Theorem} from last section.\\

\noindent Let $G_n$ be any cycle graph and let $j, k$ be any integers in $\mathbb{N}$.\\

\noindent By definition of $\Psi$, we know $\Psi(G_n, j, k)$ will be equal to the index of  $\psi(G_n, j, k)$.\\

\noindent By definition of $\psi$, we know $\psi(G_n, j, k) = \omega(p_0, j + k)$.\\

\noindent By definition of $\Omega$, the index of point $\omega(p_0, j + k)$ is given by $\Omega(p_0, j + k)$.\\

\noindent As such, we have that $\Psi(G_n, j, k) = \Omega(p_0, j + k)$.\\

\noindent We can now apply the \hyperlink{theorem:omega_walk_theorem}{Omega Walk Theorem} to complete our proof:
\begin{center}
$\Psi(G_n, j, k) = \Omega(p_0, j + k) = \rho(n, j + k + 0) = \rho(n, j + k)$.
\end{center}

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}




























\begin{tcolorbox}
\textbf{Lemma}\\
The bridge functions $\alpha$ and $\beta$ are inverses.
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent We need to show that for any $p_i$ that $\alpha(\beta(p_i)) = p_i$, and that for any $s_i$ that $\beta(\alpha(s_i)) = s_i$.\\

\noindent Let $p_i \in P$ be any point of cycle graph $G_n$.\\ 

\noindent By definition of $\beta$, $\beta(p_i) = s_i$.\\

\noindent By definition of $\alpha$, $\alpha(s_i) = p_i$.\\

\noindent Thus, $\alpha(\beta(p_i)) = \alpha(s_i) = p_i$.\\

\noindent Let $s_i \in S$ be any of state of cycle machine $C_n$.\\

\noindent Then, $\beta(\alpha(s_i)) = \beta(p_i) = s_i$

\noindent 
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}





\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle machine $C_n$, for any integer $k \in \mathbb{N^+}$,

\begin{center}
$\phi(C_n, k) = 
\alpha(\omega(\beta(\phi(C_n, k - 1)), 1))$
\end{center}
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent This will be a proof by induction on $k$.\\


\noindent
\textit{Base Case}

\noindent Let $k = 1$. We need to show $\phi(C_n, 1) = \alpha(\omega(\beta(\phi(C_n, 0)), 1))$.\\

\noindent $\phi(C_n, 1)$ is defined to be the state after transitioning once from the initial state.\\

\noindent By definition of a cycle machine the initial state, $s_0$, transitions to state $s_1$.\\

\noindent As such, $\phi(C_n, 1) = s_1$.\\

\noindent Now let us turn our attention to the left side of our equation.\\

\noindent $\phi(C_n, 0) = s_0$ by previous lemma.\\

\noindent $\beta(s_0) = p_0$ by definition of $\beta$.\\

\noindent $\omega(p_0, 1) = p_1$ by previous lemma.\\

\noindent $\alpha(p_1) = s_1$ by definition of $\alpha$.\\

\noindent As such, $\phi(C_n, 1) = s_1 = \alpha(\omega(\beta(\phi(C_n, 0)), 1))$.\\


\noindent
\textit{Inductive Hypothesis}

\noindent Assume for all $j < k$, that $\phi(C_n, j) = \alpha(\omega(\beta(\phi(C_n, j - 1)), 1))$.\\


\noindent
\textit{Inductive Step}

\noindent We need to show that $\phi(C_n, k) = \alpha(\omega(\beta(\phi(C_n, k - 1)), 1))$.\\

\noindent $\phi(C_n, k - 1) = \alpha(\omega(\beta(\phi(C_n, k - 2)), 1))$



\noindent 
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}





\noindent We are going to be interested in studying the result of carrying out multiple transitions of cycle machines. As such, let us now define two functions - the state function, and the state index function - to allow us to more succinctly talk about transitions of an arbitrary cycle machine.\\



\label{definition:state_function}
\hypertarget{definition:state_function}{}
\begin{tcolorbox}
\textbf{Definition}

\noindent For any cycle machine $C_n$ with states $S$, for any state $s_i \in S$, for any $j, k \in \mathbb{N}$, we define the \textit{state function}, $\psi(C_n, j, k)$, to be the state of cycle machine $C_n$ after completing $k$ subsequent transitions from step $j$.\\

\noindent Furthermore, we define the \textit{state position function}, $\Psi(C_n, j, k)$ to be index of the state $\psi(C_n, j, k)$.
\end{tcolorbox}
\noindent


\label{definition:state_function}
\hypertarget{definition:state_function}{}
\begin{tcolorbox}
\textbf{Definition}

\noindent For any cycle machine $C_n$ with states $S$, for any state $s_i \in S$, for any $j, k \in \mathbb{N}$, we define the \textit{state function}, $\psi(C_n, j, k)$, to be the state of cycle machine $C_n$ after completing $k$ subsequent transitions from step $j$.\\

\noindent Furthermore, we define the \textit{state position function}, $\Psi(C_n, j, k)$ to be index of the state $\psi(C_n, j, k)$.
\end{tcolorbox}
\noindent



\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle machine $C_n$, $\psi(C_n, 0, 0) = s_0$.
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent $\psi(C_n, 0, 0)$ is defined to be the state of cycle machine $C_n$ after $0$ transitions from step $0$.\\

\noindent Step $0$ is defined to be the state of cycle machine $C_n$ after taking $0$ subsequent transitions from the initial state. As such, the machine is still at it's initial state on step $0$ since it does not transition states at all.\\

\noindent As such, $\psi(C_n, 0, 0)$ is defined to be the state of the cycle machine after taking $0$ transitions from the initial state, as such, the machine is still at it's initial state.\\

\noindent The initial state of a cycle machine is defined to be $s_0$. 

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}



\begin{tcolorbox}
\textbf{Lemma}\\
For any cycle machine $C_n$ with states $S$, let $f: S \rightarrow P$ be defined as $f(s_i) = p_i$, where $P$ is the set of points of the cycle graph $G_n$.\\

\noindent Furthermore, $f$ is clearly invertible with $f^{-1}(p_i) = s_i$.\\

\noindent Then for any state $s_i$, the subsequent state is given by $f^{-1}(\omega(f(s_i), 1))$, where $f^{-1}$ is the inverse of the function $f$.\\
\end{tcolorbox}

\noindent
\textit{Proof}

\noindent Let $s_i$ be any state such that $i < n - 1$.\\

\noindent By definition of a cycle machine, we know the subsequent state of $s_i$ is defined to be $s_{i+1}$.\\

\noindent By definition of $f$, we have that $f(s_i) = p_i$.\\

\noindent As such $\omega(f(s_i), 1) = \omega(p_i, 1)$.\\

\noindent By \hyperlink{lemma:walk_1}{previous lemma}, we know $\omega(p_i, 1) = p_{i+1}$ since $i < n - 1$.\\

\noindent Thus, by substitution $f^{-1}(\omega(f(s_i), 1)) = f^{-1}(p_{i+1}) = s_{i+1}$.\\

\noindent Now we are left with showing that the transition from $s_{n-1}$ is given by $f^{-1}(\omega(f(s_{n-1}), 1))$.\\

\noindent By definition of a cycle machine, we know that the subsequent state of $s_{n-1}$ is defined to be $s_0$.\\

\noindent By definition of $f$, we have that $f(s_{n-1}) = p_{n-1}$.\\

\noindent By \hyperlink{lemma:walk_1}{previous lemma}, we know $\omega(p_{n-1}, 1) = p_0$.\\

\noindent Thus, by substitution $f^{-1}(\omega(f(s_{n-1}), 1)) = f^{-1}(p_0) = s_0$.\\

\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}

\section{Activation Cycle Machines}
With a few, minor modifications to cycle graphs, we can create \textit{activation cycle machines}; and in doing so, bring ourselves one step closer to understanding awkward state machines. So what exactly is an activation cycle machine? Let us answer that question by first looking at the formal definition; and then taking some time to examine its meaning afterward.\\
\\
\begin{tcolorbox}
\textbf{Definition}\\
An \textit{activation cycle machine} is a state machine with $n > 1$ states, $s_0, s_1, ..., s_{n-1}$, such that for any state, $s_k$ where $k < n - 1$, state $s_k$ has only a single transition to state $s_{k+1}$; and state $s_{n-1}$ has only a single transition to $s_0$. Furthermore, all activation cycle machines always start at state $s_0$.\\
\\
The first $a$ states, where $1 \leq a < n$, of an activation cycle machine are called \textit{activators}. An activation cycle machine is said to be \textit{active} when it's current state is one of it's activators, and is called \textit{inactive} when it's not active. We write $C_{n,a}$ to refer to the activation cycle machine with $n$ states and $a$ activators.\\
\\
When an activation cycle machine's current state is $s_k$, for any $k$, we call $k$ its \textit{position}, and write $\overline{C}(j)$ to refer to the position of the machine after it's $j^{th}$ transition. 
\end{tcolorbox}
\noindent
\\
The first part of our definition is almost exactly the same as our definition for a cycle graphs. However, instead of talking about points in a graph, and the edges between those points; we define the states of a machine, and the transitions between those states. With such similar definitions, it seems reasonable that we can represent the states of an activation cycle machine by using a cycle graph. In fact, we'll begin our study of activation cycle machines by proving just that.\\
\\
The definition of an activation cycle machine extends a bit beyond that of cycle graphs by incorporating the notion of \textit{activators},  which is merely a label applied to the first $a$ states of the machine, $s_0, ..., s_{a-1}$. Furthermore, our definition stipulates that while there is always at least one activator, there are never as many activators as there are states in the machine. As such, every activation cycle machine will become \textit{active} at least once, as well as \textit{inactive} at least once as it transitions between its states.\\
\\
Before we dive into our study of activation cycle machines, below you will find two working examples of an activation cycle machine coded in \textit{ruby}. The first example is more condensed. Rather than keep track of states directly, the first example simulates an activation cycle machine by keeping track of the current position of the machine using arithmetic. Our second example, on the other hand, is represented using states, and as such, needn't rely on arithmetic to function. Whether or not you program, I encourage you to look through both examples carefully; I think you will find they provide a bit of color to our abstract definition.
\begin{tcolorbox}
\begin{verbatim}
class ActivationCycleMachine
  def initialize(number_of_states, number_of_activators)    
    unless number_of_states > 1
      raise 'There must be more than one state'
    end
    
    unless number_of_activators.positive?
      raise 'The number of activators must be positive'
    end
    
    unless number_of_activators < number_of_states
      raise 'The number of activators must be less ' \
            'than the number of states'
    end
    
    @number_of_states = number_of_states
    @number_of_activators = number_of_activators
    @position = 0
  end

  def next_state
    if @position == @number_of_states - 1
      @position = 0
    else
      @position = @position + 1
    end
  end
  
  def active?
    @position < @number_of_activators
  end
  
  def inactive?
    !active?
  end
  
  def position
    @position
  end
end
\end{verbatim}
\end{tcolorbox}
\noindent
\\
As you can see, our first example simply increments the position of our state machine by one to simulate a transition, with the exception that it resets the position to 0 once its gotten to the maximum position. Since this example does not use states, it does not have a direct understandg of an \textit{activator}. Despite that, the implementation is able to determine activeness indirectly by checking if it's current position is less than the number of activators.\\
\\
We'll need to break our second example into two seperate classes. Within the first class, we will represent a single state in our machine, which will be implemented as a node in a linked list would. Our second class will be the representation of the activation cycle machine, which we implement much the same way that we would a linked list.\\
\\
\begin{tcolorbox}
\begin{verbatim}
class State
  def initialize(is_activator, position)
    @is_activator = is_activator
    @position = position
  end

  def activator?
    @is_activator
  end
  
  def next_state=(next_state)
  	@next_state = next_state
  end
  
  def next_state
  	@next_state
  end
  
  def position
  	@position
  end
end
\end{verbatim}
\end{tcolorbox}
\begin{tcolorbox}
\begin{verbatim}
class ActivationCycleMachine
  def initialize(number_of_states, number_of_activators)
    unless number_of_states > 1
      raise 'There must be more than one state'
    end

    unless number_of_activators.positive?
      raise 'The number of activators must be positive'
    end

    unless number_of_activators < number_of_states
      raise 'The number of activators must be less ' \
            'than the number of states'
    end

    initial_state = State.new(true, 0)
    current_state = initial_state
    (1...number_of_states).each do |i|
      activator = i < number_of_activators
      current_state.next_state = State.new(activator, i)
      current_state = current_state.next_state
    end
    current_state.next_state = initial_state

    @current_state = initial_state
  end

  def next_state
    @current_state = @current_state.next_state
  end

  def active?
    @current_state.activator?
  end

  def position
    @current_state.position
  end
end
\end{verbatim}
\end{tcolorbox}
\noindent
\\
As you can see, our second example matches our formal mathematical definition very closely. In particular, the activation cycle machine, once initialized, only has a notion of a current state. The machine is operated by simply replacing the current state with its next state, thus simulating a transition. Furthermore, since the states themselves encode whether or not its an activator; the activation cycle machine is able to determine if its active by simply asking if its current state is an activator; matching our mathematical definition exactly.\\
\\
Without further ado, let us now tie activation cycle machines and cycle graphs together.\\
\begin{tcolorbox}
\textbf{Lemma}\\
Let $C_{n,a}$ be any activation cycle machine.\\
Let $S = \{$ $s_i$ $|$ $0 \leq i < n$ $\}$ be the states of $C_{n,a}$.\\
\\
Let $G_n$ be the cycle graph of $n$ points.\\
Let $P = \{$ $p_j$ $|$ $0 \leq j < n$ $\}$ be the points of $G_n$.\\
\\
Let $\gamma: P \rightarrow S$ be the mapping from the points in our cycle graph to the states in our activation cycle machine such that $\gamma(p_k) = s_k$.\\
\\
Let $\theta: S \rightarrow P$ be the mapping from the states of our activation cycle machine to the points in our graph such that $\theta(s_k) = p_k$.\\
\\
Let $\omega: (P, \mathbb{N}) \rightarrow P$ such that $\omega(p_i, k) = p_j$, where $p_j$ is the point arrived to after a walk of length $k$.\\
\\
Let $\sigma: (S, \mathbb{N}) \rightarrow S$ such that $\sigma(s_i, k) = s_j$, where $j$ is the position of the machine after $k$ transistions when starting at state $s_i$.\\  
\\
Then $\gamma(\omega(p_i, 1)) = \sigma(s_i, 1)$.\\
\\
In other words, for any activation cycle machine, we can determine the next state to be transitioned to by instead taking a walk of length one, starting at the point with the same index as the starting state, around the cycle graph with the same number of points as states in our machine; and then finally mapping the resulting point from our walk back to the states in our machine using $\gamma$.
\end{tcolorbox}
\noindent
\textit{Proof}\\
Our proof is rather trivial and will come directly from our definitions. Let us assume we have some activation cycle machine with $n$ states, $S = \{s_0, s_1, ..., s_{n-1}\}$, and the cycle graph with $n$ points, $P = \{p_0, p_1, ..., p_{n-1}\}$. Let us define the mapping $\theta: S \rightarrow P$ as $\theta(s_k) = p_k$, and it's inverse $\gamma: P \rightarrow S$ which is clearly $\gamma(p_k) = s_k$.\\
\\
To begin, we will show that if our activation cycle machine is on any state, $s_k$, that we can determine the next state our machine will transition to by taking a walk of length $1$ starting at point $\theta(s_k) = p_k$ and mapping the resulting point, $p_j$, back to state $\gamma(p_j) = s_j$.\\
\\
Let our activation cycle machine be at position $n - 1$. If we were to take a walk of length $1$ from point $\theta(s_{n-1}) = p_{n-1}$, then we will arrive at point $p_0$ by definition of a cycle graph. Mapping point $p_0$ back to our cycle machine under $\gamma$ yields state $\gamma(p_0) = s_0$. By definition of an activation cycle machine, we know that state $s_{n-1}$ transitions to state $s_0$, thus our mappings hold for $n - 1$.\\
\\
Now let us assume our activation cycle machine is at position $k < n - 1$. If we were to take a walk of length $1$ from point $\theta(s_k) = p_k$, then we will arrive at point $p_{k+1}$ by definition of a cycle graph. Mapping point $p_{k+1}$ back to our cycle machine under $\gamma$ yields state $\gamma(p_{k+1}) = s_{k+1}$. By definition of an activation cycle machine, we know that state $s_k$ transitions to state $s_{k+1}$, thus our mappings hold for all $k$.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}
\noindent
\\
With the proof of this simple lemma, we can immediately apply all our knowledge of cycle graphs to activation cycle machines. Most importantly, we gain the ability to predict the position of any activation cycle machine after any number of transitions.\\
\begin{tcolorbox}
\textbf{Corollary}\\
For any $n$, for any $k$, the position of the activation cycle machine with $n$ states after $k$ transitions starting from any position $j$ can be determined by taking a walk of length $k$ starting from point $p_j$ around the cycle graph with $n$ points.
\end{tcolorbox}
\noindent
\textit{Proof}\\
Since we can predict the resulting state of a transition of our cycle machine by instead taking a walk of length $1$ on our corresponding cycle graph, it should be obvious that we can also predict the resulting state of our cycle machine after $k$ transitions by instead taking a walk of length $k$ on our cycle graph.\\
\\
We shall prove our corollary by induction on $k$, the number of transitions our state machine undergoes. Our base case, $k = 1$, is the direct result of our previos lemma. As such, let us assume for all $j \leq k$, that the resulting position of our cycle machine after $j$ transitions from any position, $i$, can be determined by instead taking a walk of length $j$ starting at point $p_i$ on our cycle graph and mapping the resulting point back to our cycle machine under our mapping $\gamma: P \rightarrow S$, $\gamma(p_l) = s_l$.\\
\\
To transition $k + 1$ states, let us first transition $k$ states from our position, $i$. By assumption, we know that our resulting state will be given by taking a walk from point $p_i$ of length $k$ and mapping the resulting point, $p_l$, back to our machine under $\gamma$, giving us state $\gamma(p_l) = s_l$. We now only have one transition left to complete our $k + 1$ transitions. By our lemma, we know this final transition from our state $s_l$ is given by taking extending our walk one more step from $p_l$ and mapping the result back to our machine. As such, mapping the result of a walk of length $k + 1$ around our cycle graph does in fact result in the same position as $k + 1$ transitions would have.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}
\noindent
\\
\begin{tcolorbox}
\textbf{Corollary}\\
For any $n$, for any $k$, the position of the activation cycle machine with $n$ states after $k$ transitions starting from any position $j$ is given by $(j + k)$ $mod$ $n$.
\end{tcolorbox}
\noindent
\textit{Proof}\\
By our previous corollary, we know the position of our cycle machine after $k$ transitions starting at any position, $j$, can be determined by instead taking a walk of length $k$ starting from point $p_j$ around the cycle graph with $n$ points.\\
\\
By our previous theorem on cycle graphs, we know our walk of length $k$ will end at point $p_i$, where $i = (j + k)$ $mod$ $n$. As such, the position of our cycle graph after $k$ transitions will also be $i$.
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}
\noindent
\begin{tcolorbox}
\textbf{Lemma}\\
For any activation cycle machine $C_{n,a}$, for any $k$, the activation cycle machine is active if and only if $a > (k$ $mod$ $n)$.
\end{tcolorbox}
\noindent
\textit{Proof}\\
By our previous corollary, we know that the position of our cycle machine, $C_{n,a}$, after $k$ transitions will be given by $j =(k + i)$ $mod$ $n$, where $i$ is the initial position of our machine. By definition of an activation cycle machine, we know that $i = 0$, thus, our position after the first $k$ transitions is given by $j = k$ $mod$ $n$.\\
\\
Furthermore, an activation cycle machine is defined to be active whenever its current state is one of its activators, in other words, whenever its position is less than $a$. Putting the two together, we get that our cycle machine is active after the initial $k$ transitions whenever $a > k$ $mod$ $n$
\begin{center}
\noindent\rule{8cm}{0.4pt}
\end{center}
  
\section{Awkward State Machines}

Now that we have our definitions for cycle graphs and ACMs, we are almost ready to formally define the \textit{awkward state machine} or \textit{ASM}. An ASM is a state machine for the purpose of generating a set of ACMs using a pre-existing set of ACMs. In order to accomplish this task, with each step of an ASM, all the pre-existing ACMs state's are incremented by one; if none of the ACMs are active after moving, then a new ACM is added to the ASM and it's position is set to 0. Thus, every state of an ASM has at least one ACM that is active.\\
\\
Every ASM starts with an initial ACM set to position 0, and an \textit{activation branch} of length one greater than the ACM. An activation branch is simply an ACM that has the edge removed from its last point to its inital point; thus an activation branch forms a line instead of a circle. Whenever the ASM must create a new ACM, it does so by creating a copy of its activation branch, then adds the missing edge to the original branch in order to form the new ACM required by the machine.  Furthermore, a new node is added to the end of the copied activation branch. If the ASM does not need to create a new ACM after moving, then a new node is added to the end of its activation branch. Thus, the length of an ASM's activation branch increases by one with every iteration.\\
\\
This section will explore several fundamental questions about ASMs and the sets of ACMs they generate. We will find that the sets of ACMs generated by ASMs have a unique and unexpected relationship to modular arithmetic. For instance, we will see that the most basic ASM generates the prime numbers; and all other ASMs generate sets of integers just as strange, or awkward, as the primes. Furthermore, in our final theorem of this section, we will show that every ASM will generate an infinite set of ACMs if left running.\\   
\\
We will begin this section by formally defining the activation branch and proving that we can indeed create an ACM by adding an edge to it. From there, we will formally define ASMs and then begin our exploration.\\
\\
\\
\textbf{Definition}\\
An \textit{activation branch} is directed graph that forms a line. Explicity, the activation branch of length $n$ has points $\{$ $p_0, p_1 ..., p_{n-1}$ $\}$ such that for every point $p_{i < n -1}$ has a single edge connecting it to $p_{i+1}$, and $p_{n-1}$ does not have any edges extending from it.\\
\\
Furthermore, an activation branch is equipped with $a > 0$ activator points: $A = \{$ $p_i$ $|$ $0 \leq i < a < n$ $\}$.\\ 
\\
We denote the activation branch with length $n$ and $a$ activator nodes $B_{n,a}$.
\\
\\
\textbf{Lemma}\\
For any $B_{n,a}$, adding an edge extending from $p_{n-1}$ to $p_0$ produces the cycle graph $CG_n$ for ACM $C_{n,a}$.\\
\\
\textit{Proof}\\ 
By definition of $B_{n,a}$, all points $p_{i < n -1}$ have the same edges as the first $n - 1$ points in $CG_n$ for ACM $C_{n,a}$.\\
\\
Furthermore, the first $a$ points of $B_{n,a}$ are it's activator points, which are the activators of $C_{n,a}$.\\
\\
Thus, the only edge missing from $B_{n,a}$ that is contained within $CG_n$ is from  $p_{n-1}$ to $p_0$.\\
\\
Therefore, adding the edge will convert $B_{n,a}$ into $CG_n$ for $C_{n,a}$.\\
\textit{Q.E.D}\\
\\
\textbf{Definition}\\
An \textit{awkward state machine} (ASM) is state machine running on top of a directed graph composed of a set of ACMs and a single activation branch.\\
\\
Every ASMs initial state contains a single $ACM$, $C_{m,a} = C_0$, and the activation branch, $B_{m+1,a}$.\\
Furthermore, the position of $C_0$ on the initial state is $0$.\\
We denote the ASM with an initial state containing $C_{m,a}$ as $S_{a,n=m-a}$.\\
\\
To progress from state $i - 1$ to state $i$ for ASM, $S$, first move every ACM in $S$ to it's next state.\\
If none of the ACMs in $S$ are active after moving to their next state, then:
\begin{enumerate}
\item Create a copy of the activation branch, giving you branches $B$ and $B`$.
\item Convert $B$ into an $ACM$, $C$, by adding an edge to it's last point, thus leaving a single activation branch $B`$. Set the position of $C$ to $0$. Now $C$ is contained within the set of ACMs for $S$.
\end{enumerate}
Regardless of whether one of the ACMs were active, add a new point to the end of the activation branch. Explicitly, if the activation branch had length $n$, then add an edge extending from $p_{n-1}$ to the new point $p_n$, thus creating an activation branch of length $n + 1$.\\
\\
If none of the ACMs were active after moving them to their next state, we say that $S$ is \textit{inactive} after moving; otherwise we say $S$ is \textit{active} after moving. Therefore, we only add a new ACM to $S$ when $S$ is inactive after moving.\\
\\
We denote the inition cycle of an ASM $C_0$, the first discovered cycle $C_1$, and the $n$th discovered cycle $C_n$.\\
\\
We denote the graph of ASM $S_{a,n}$ on state $k$ as $S^k$.\\
\\
If $C_{p,a}$ is discovered by ASM $S_{a,n}$ on some step $k$, then we say that $C_{p,a}$ is \textit{discoverable} by $S$.\\
\\
\textbf{Definition}\\  
We'll define $[S^a] = \{$ $C_i$ within the graph of $S$ on step $a$ $\}$,\\
and $[S] = \{$ $C_0$ and all $C_i$ disoverable by $S$ $\}$. We refer to $[S]$ as the \textit{school} of cycles for $S$.\\
\\
\\
\textbf{Lemma}\\
For $S_{a,n}$, the length of the branch, $B$, on step $k$ is given by $|B^k| = k + a + n + 1$.\\
\\
\textit{Proof}\\  
One step $0$, the length of branch $B$ is given by $|B^0| = 0 + a + n + 1$.\\
With each step, a single node is added to the branch $B$. Thus, $|B^{i+1}| = |B^{i}| + 1$.\\
\\
Assume $|B^{i}| = i + a + n + 1$.\\
Then $|B^{i+1}| = |B^{i}| + 1 = (i + a + n + 1) + 1 = (i + 1) + a + n + 1$.\\
\\
Thus we have shown that $|B^k| = k + a + n + 1$ by induction.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Lemma}\\
For $S_{a,n}$, if $C_i$ is discovered on step $k$, then the length of $|C_i| = |B^{k-1}| = k + a + n$.\\
\\
Assume for $S_{a,n}$, that $C_i$ is discovered on step $k$.\\
\\
To produce state $k$, the ASM algorithm first moved all $C_j$ for $j < i$ from state $C^{k-1}$ to state $C^k$. After doing so, there did not exist a $j < i$ such that $C_j$ was active. Therefor, the algorithm copied the branch $B^{k-1}$ and closed one of the two branches to create $C_i$.\\
\\ 
Thus, the length of $C_i$ is equal to the length of the branch $B^{k-1}$:\\ $|C_i| = |B^{k-1}| = (k - 1) + a + n + 1 = k + a + n$.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Lemma}\\
For $S_{a,n}$, $|C_{i}| \geq |C_{i-1}| + a$.\\
\\
\textit{Proof}\\  
Assume cycle $C_i$ is discovered by $S_{a,n}$ on step $k$.\\
\\
Then $\overline{C^k_i} = 0$.\\
Furthermore, the new branch, $B$, will have $|C_i| + 1$ nodes at step $k$.\\
\\
For next, $1 \leq j < a$ steps, the cycle  $C^{k+j}_i$ will be active since there are $a$ activation nodes.\\
Furthermore, the length of the branch $B$ at each step will be given by $|B| = |C_i| + j + 1$.\\
\\
The $a$th step after discovering $C_i$ will be the first time that $C_i$ will be inactive. The branch length of the $(a-1)$th step is given by $|B^{j + a - 1}| = |C_i| + a$. Thus, if $C_p$ for $0 \leq p < i$ are also inactive on step $a$th step after discovering $C_i$, then we will have to close $B$ on the $a$th step, thus creating cycle $C_{i+1}$ with length $|C_{i+1}| = |B^{j + a - 1}| = |C_i| + a$. Furthermore, if any of the cycles $C_p$ were active on the $a$th step after discovering $C_i$, then the branch would not close, thus the next cycle's length is at least as long as the branch on the $(j+a)$th step: $|C_{i+1}| >= |B^{j + a}| = |C_i| + a + 1$.\\
\\
Thus, we have shown that  $|C_{i}| \geq |C_{i-1}| + a$.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Lemma}\\
For any $C_i, C_j \in [S_{a,n}]$ with $j < i$, it holds that $|C_{i}| \geq |C_j| + (i - j)a$.\\
\\
\textit{Proof}\\ 
If $i = j + 1$, then $j - i = 1$.\\
Thus, $|C_i| \geq |C_j| + a = |C_j| + (j - i)a$.\\
\\
Assume for $k$, $i < k \leq j$, that $|C_k| \geq |C_i| + (k-i)a$.\\
\\
Then $|C_{k+1}| \geq |C_k| + a \geq (|C_i| + (k-i)a) + a = |C_i| + (k + 1 - i)a$.\\
\\
Thus we have shown that $|C_{i}| \geq |C_j| + (i - j)a$ by induction.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Corollary}\\
For any cycle $C_{j}$ of an ASM, $S_{a,n}$, $C_j$ has at least $n + ja$ non-activator nodes.\\
\\
\textit{Proof}\\
$C_0$ is defined to have $n$ activator nodes.\\
\\
Let $C_{j>0} \in [S_{a,n}]$.\\
\\
Then $|C_j| \geq |C_1| + (j-1)a \geq (|C_0| + a) + (j-1)a $\\
$= (2a + n) + (j-1)a = n + (j+1)a$.\\
\\
Thus, after removing the $a$ activator nodes from $C_j$, there are at least $n + ja$ non-activator nodes left.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Lemma}\\
Every ASM discovers at least one cycle.\\
\\
\textit{Proof}\\
Let $C_0$ by the initial cycle for ASM, $S_{a,n}$.\\
\\
After taking $a$ steps from the initial state of the $S$, the ASM will be on it's first non-activator node.\\
\\
Since $C_0$ is the only cycle, the ASM would be inactive, thus a new cycle would be created.\\
\textit{Q.E.D}\\
\\ 
\\
\textbf{Lemma}\\
For any step, $k$, for any $C_i \in [S^k_{a,n}]$, it holds that $\overline{C^k_i} = (k + a + n)$ $mod$ $|C_i|$.\\
\\
\textit{Proof}\\  
The initial cycle $C_0$ starts at position $0$ on step $0$. By properties of ACM's, we know that the position of $C_0$ is given by:\\
\\
$\overline{C^k_0} = k$ $mod$ $|C_0| $
$= [k +(a + n)]$ $mod$ $(a + n)$
$= (k + a + n)$ $mod$ $|C_0|$.\\
\\
If a cycle, $C_i$, is discovered on step $k$, then we know it's length is given by $|C_i| = k + a + n$. Furthermore, when it's dicovered, it's position is set to 0.\\
\\
Thus, the position of $C^k_i$ is given by:\\
\\
$\overline{C^k_i} = 0 = (k + a + n)$ $mod$ $(k + a + n) = (k + a + n)$ $mod$ $|C_i|$.\\
\\
By properties of ACM's, the position for all steps $j > k$ will be given by:\\
\\
$\overline{C^j} = (\overline{C^k} + (j - k))$ $mod$ $|C^i|$
$= (0 + (j-k))$ $mod$ $(k + a + n)$\\
$= (k + a +n) + (j-k)$ $mod$ $|C_i| $
$= (j + a + n)$ $mod$ $|C_i|$.\\
\\
Therefor we have shown that the position of $C_i$ on step $k$ for any $C_i \in [S^k_{a,n}]$ is given by $\overline{C^k_i} = (k + a + n)$ $mod$ $|C_i|$ for any $C_i$ in $[S^k]$.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Corollary}\\
For any cycle $C_j \in [S_{a,n}]$, moving $k|C_j|$ steps will maintain $C_j$'s position.\\
\\
\textit{Proof}\\
Let $C_j \in [S]$.\\
Assume $S$ is on step $p$.\\
\\
Then $\overline{C_j} = p + a + n $ $mod$ $|C_j|$.\\
Thus, if we move $k|C_j|$ steps from $p$, the position of $C_j$ will be given by:\\
\\
$p + a + n + k|C_j|$ $mod$ $|C_j| = p + a + n$ $mod$ $|C_j|$.\\
\\
Therefor $C_j$'s position was maintained.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Lemma}\\
For any cycle $C \in [S]$, if $k \geq |C|$, then the position of $C$ after moving $k$ steps from $C$'s discovery is the same as moving $k$ $mod$ $|C|$ steps.\\
\\
\textit{Proof}\\
Assume $C \in [S]$, and $S$ is on step $p$.\\
Then the position of $C$ is given by $p + a + n$ $mod$ $|C|$.\\
\\
Let $k > |C|$, $b = k$ $mod$ $|C|$.\\
\\
The position of $C$ on step $p + k$ is given by:\\
$p + k + a + n$ $mod$ $C = p + b + a + n$ $mod$ $C$ which is the position after moving $b$ steps from $p$.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Lemma}\\
For any $C_i, C_j \in [S_{a,n}]$ with $j < i$, it holds that $|C_i|$ $mod$ $|C_j| \geq a$.\\
\\
\textit{Proof}\\  
Assume for ASM, $S_{a,n}$, that ACM, $C_i$, is discovered on step $k$.\\
\\
Then $|C_i| = k + a + n$.\\
We know also know that the position of $C^k_j$ must be greater than or equal to $a$; otherwise, $C_j$ would be active on step $k$ and we would not have discovered $C_j$.\\
\\
Furthermore, we know the position of $C^k_j$ is given by:\\
$\overline{C^k_j} = (k + a + n)$ $mod$ $|C_j|$.\\ 
\\
Finally, subsitituting gives us: $\overline{C^k_j} = |C_i|$ $mod$ $|C_j| \geq a$.\\
\\
Thus we have shown that, $|C_i|$ $mod$ $|C_j| \geq a$.\\
\textit{Q.E.D}\\ 
\\
\\
\textbf{Lemma}\\
For any $C_i \in [S_{a,n}]$, $|C_i|$ is the least positive integer greater than $|C_{i-1}|$ such that $|C_i|$ $mod$ $|C_j| \geq a$ for all $j < i$.\\
\\
\textit{Proof}\\
Assume you discovered $C_{i-1}$ on step $p$.\\
Assume $k$ is the first step after discovering $C_{i-1}$ such that all cycles are inactive (if we can show that cycle $C_i$ is discovered on step $k$, then our proof will be complete).\\
\\
Then, on all steps, $q$, $p \leq q < k$, there must exist at least one cycle $C_j$ that is active (implying $q + a + n$ $mod$ $|C_j| < a$). Therefore, the ASM algoirthm cannot create cycle $C_i$ on any step $p \leq q < k$.\\
\\
Furthermore, on step $k$, the algorithm must create $C_i$, giving us $\overline{C^k_i} = 0 = k + a + n$ $mod$ $|C_i|$.\\
\\
Thus, since $k + a + n = |C_i|$ is the least positive integer such that $|C_i|$ $mod$ $|C_j| \geq a$ for all $j < i$, and have proved our lemma.\\
\textit{Q.E.D}\\
\\
\\
\textbf{Definition}\\
A positive integer is prime if it's only factors are 1 and itself.\\
\\
\textbf{Axiom}\\
The $i$th prime number, $p_i$, is the least integer that is greater than the $(i-1)th$ prime number and is not divisible by $p_j$ for all $j < i$ (with the first prime number $p_0=2$).\\ 
Note: This is only an axiom because proving the statement is distracting to ASMs.\\
\\
\\
\textbf{Lemma}\\
$|S_{1,1}| = P = \{$ the set of prime numbers $\}$\\
\\
\textit{Proof}\\
$S_{1,1}$ starts with an initial cycle of length $1 + 1 = 2$, thus giving us the first prime number.\\
\\
For all $i, j$ such that $i > j \geq 0$, $C_i$ will have the property that $|C_i|$ $mod$ $|C_j| \geq 1$, equivalenty, $|C_i|$ is not divisible by any of the previous lengths $|C_j|$. Furthermore, $|C_i|$ is the smallest such integer greater than $|C_{i-1}|$ with the property of not being divisible by the previous cycle lengths.\\
\\
Thus, by our axiom, $S_{1,1}$ produces the prime numbers.\\
\textit{Q.E.D}\\ 
\\
\\
\textbf{Theorem}\\
For every ASM, $S$, the set $[S]$ is non-finite.\\
\\
\textit{Proof}\\  
Assume there exists an ASM, $S_{a,n}$ such that $[S_{a,n}]$ is finite.\\
Let $s$ be the number of cycles in $[S_{a,n}]$.\\
\\
Then there exists some step $p$ on which the last cycle, $C_{max}=C_{s-1}$ of the ASM is discovered.\\
\\
We know $C_{max}$ must have at least $n + (s-1)a > (s-1)a$ non-activator nodes.\\
\\
We also know that on step $p$, all cycles $C_{i<s}$ must have been inactive to have discovered $C_{max}$.\\
\\
Furthermore, on step $j$, $C_{max}$'s position is at 0.\\
\\
Let $z = \prod_{i<s}|C_i|$.\\
\\
If we move the $ASM$ $z$ steps, then the position of every $C_{i<z}$ will be maintained since $z$ is a multiple of the length of every $C_{i<z}$.\\
\\
Thus, if after moving $z$ steps from $p$, $C_{max}$ is inactive, then we would need to create a new cycle, and we'd be finished with our proof.\\
\\
Assume $C_{max}$ was active after moving the $z$ steps.\\
\\
Let $n =\overline{C_{max}}$\\
\\
We know $n < a$ since $C_{max}$ is inactive.\\
\\
Note: the position of $C_{max}$ after moving $zt$ steps is the same as after moving $nt$ steps since $z > |C_{max}|$ (if z were less than $|C_{max}|$, then moving $z$ steps would have inactivated $C_{max}$ since $z > a$).\\
\\
Let $j$ be the greatest integer such that $jn < a$.\\
Then your position after moving $jz$ steps will be $a - jn$.\\
Thus, the $(j+1)z$th step will put $C_{max}$ at position $(a-jn)+n \geq a$.\\   
\\
If the $ASM$ is to remain active, then $(a-jn)+n$ must go beyond all $(s-1)a$ non-activator nodes in $C_{max}$.\\
\\
Thus, $(a-jn)+n > a + (s-1)a \geq sa \geq 2a$.\\
\\
But both $n$ and $(a-jn)$ are less then $a$.\\
But that means: $a + a = 2a > (a-jn)+n \geq 2a$, which is a contradiction!\\
\\
Therefore, moving $(j+1)z$ steps from the discovery of $C_{max}$ will maintain the (inactive) positions of all $C_{i<z}$ and also move $C_{max}$ into one of it's inactive nodes, requiring a new cycle to be made, and completing our proof.\\
\textit{Q.E.D}\\   
\\
\\
\textbf{Corollary}\\
There are an infinite number of prime numbers.\\
\\
\textit{Proof}\\ 
$S_{1,1}$ produces the prime numbers, and $[S_{1,1}]$ is non-finite by the above theorem.\\
\textit{Q.E.D}
\end{document}